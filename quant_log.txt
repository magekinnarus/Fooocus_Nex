main: build = 3962 (c8c07d658)
main: built with MSVC 19.42.34435.0 for x64
main: quantizing 'H:\webui_forge_cu121_torch21\webui\models\Stable-diffusion\quantization_target\fp16\dutch_v30_F16.gguf' to 'H:\webui_forge_cu121_torch21\webui\models\Stable-diffusion\quantization_target\quantized\dutch_v30_Q5_K_M_debug_rebuild.gguf' as Q5_K_M
llama_model_loader: loaded meta data with 3 key-value pairs and 1680 tensors from H:\webui_forge_cu121_torch21\webui\models\Stable-diffusion\quantization_target\fp16\dutch_v30_F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = sdxl
llama_model_loader: - kv   1:               general.quantization_version u32              = 2
llama_model_loader: - kv   2:                          general.file_type u32              = 1
llama_model_loader: - type  f16: 1680 tensors
[   1/1680]                input_blocks.0.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[   2/1680]              input_blocks.0.0.weight - [    3,     3,     4,   320], type =    f16, size =    0.022 MB
[   3/1680]   input_blocks.1.0.emb_layers.1.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[   4/1680] input_blocks.1.0.emb_layers.1.weight - [ 1280,   320,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB
[   5/1680]    input_blocks.1.0.in_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[   6/1680]  input_blocks.1.0.in_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[   7/1680]    input_blocks.1.0.in_layers.2.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[   8/1680]  input_blocks.1.0.in_layers.2.weight - [    3,     3,   320,   320], type =    f16, size =    1.758 MB
[   9/1680]   input_blocks.1.0.out_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[  10/1680] input_blocks.1.0.out_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[  11/1680]   input_blocks.1.0.out_layers.3.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[  12/1680] input_blocks.1.0.out_layers.3.weight - [    3,     3,   320,   320], type =    f16, size =    1.758 MB
[  13/1680]   input_blocks.2.0.emb_layers.1.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[  14/1680] input_blocks.2.0.emb_layers.1.weight - [ 1280,   320,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB
[  15/1680]    input_blocks.2.0.in_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[  16/1680]  input_blocks.2.0.in_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[  17/1680]    input_blocks.2.0.in_layers.2.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[  18/1680]  input_blocks.2.0.in_layers.2.weight - [    3,     3,   320,   320], type =    f16, size =    1.758 MB
[  19/1680]   input_blocks.2.0.out_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[  20/1680] input_blocks.2.0.out_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[  21/1680]   input_blocks.2.0.out_layers.3.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[  22/1680] input_blocks.2.0.out_layers.3.weight - [    3,     3,   320,   320], type =    f16, size =    1.758 MB
[  23/1680]             input_blocks.3.0.op.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[  24/1680]           input_blocks.3.0.op.weight - [    3,     3,   320,   320], type =    f16, size =    1.758 MB
[  25/1680]   input_blocks.4.0.emb_layers.1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  26/1680] input_blocks.4.0.emb_layers.1.weight - [ 1280,   640,     1,     1], type =    f16, converting to q5_K .. size =     1.56 MiB ->     0.54 MiB
[  27/1680]    input_blocks.4.0.in_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[  28/1680]  input_blocks.4.0.in_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[  29/1680]    input_blocks.4.0.in_layers.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  30/1680]  input_blocks.4.0.in_layers.2.weight - [    3,     3,   320,   640], type =    f16, size =    3.516 MB
[  31/1680]   input_blocks.4.0.out_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  32/1680] input_blocks.4.0.out_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  33/1680]   input_blocks.4.0.out_layers.3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  34/1680] input_blocks.4.0.out_layers.3.weight - [    3,     3,   640,   640], type =    f16, size =    7.031 MB
[  35/1680] input_blocks.4.0.skip_connection.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  36/1680] input_blocks.4.0.skip_connection.weight - [    1,     1,   320,   640], type =    f16, size =    0.391 MB
[  37/1680]           input_blocks.4.1.norm.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  38/1680]         input_blocks.4.1.norm.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  39/1680]        input_blocks.4.1.proj_in.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  40/1680]      input_blocks.4.1.proj_in.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  41/1680]       input_blocks.4.1.proj_out.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  42/1680]     input_blocks.4.1.proj_out.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  43/1680] input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  44/1680] input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  45/1680] input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  46/1680] input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  47/1680] input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.78 MiB ->     0.42 MiB
[  48/1680] input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB
[  49/1680] input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  50/1680] input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  51/1680] input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  52/1680] input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q6_K .. size =     2.50 MiB ->     1.03 MiB
[  53/1680] input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB
[  54/1680] input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight - [  640,  5120,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 5120 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     6.25 MiB ->     2.34 MiB
[  55/1680] input_blocks.4.1.transformer_blocks.0.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  56/1680] input_blocks.4.1.transformer_blocks.0.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[  57/1680] input_blocks.4.1.transformer_blocks.0.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  58/1680] input_blocks.4.1.transformer_blocks.0.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  59/1680] input_blocks.4.1.transformer_blocks.0.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  60/1680] input_blocks.4.1.transformer_blocks.0.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  61/1680] input_blocks.4.1.transformer_blocks.0.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  62/1680] input_blocks.4.1.transformer_blocks.0.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  63/1680] input_blocks.4.1.transformer_blocks.1.attn1.to_k.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  64/1680] input_blocks.4.1.transformer_blocks.1.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  65/1680] input_blocks.4.1.transformer_blocks.1.attn1.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  66/1680] input_blocks.4.1.transformer_blocks.1.attn1.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  67/1680] input_blocks.4.1.transformer_blocks.1.attn1.to_v.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.78 MiB ->     0.42 MiB
[  68/1680] input_blocks.4.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB
[  69/1680] input_blocks.4.1.transformer_blocks.1.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  70/1680] input_blocks.4.1.transformer_blocks.1.attn2.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  71/1680] input_blocks.4.1.transformer_blocks.1.attn2.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  72/1680] input_blocks.4.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q6_K .. size =     2.50 MiB ->     1.03 MiB
[  73/1680] input_blocks.4.1.transformer_blocks.1.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB
[  74/1680] input_blocks.4.1.transformer_blocks.1.ff.net.0.proj.weight - [  640,  5120,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 5120 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     6.25 MiB ->     2.34 MiB
[  75/1680] input_blocks.4.1.transformer_blocks.1.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  76/1680] input_blocks.4.1.transformer_blocks.1.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[  77/1680] input_blocks.4.1.transformer_blocks.1.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  78/1680] input_blocks.4.1.transformer_blocks.1.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  79/1680] input_blocks.4.1.transformer_blocks.1.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  80/1680] input_blocks.4.1.transformer_blocks.1.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  81/1680] input_blocks.4.1.transformer_blocks.1.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  82/1680] input_blocks.4.1.transformer_blocks.1.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  83/1680]   input_blocks.5.0.emb_layers.1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  84/1680] input_blocks.5.0.emb_layers.1.weight - [ 1280,   640,     1,     1], type =    f16, converting to q5_K .. size =     1.56 MiB ->     0.54 MiB
[  85/1680]    input_blocks.5.0.in_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  86/1680]  input_blocks.5.0.in_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  87/1680]    input_blocks.5.0.in_layers.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  88/1680]  input_blocks.5.0.in_layers.2.weight - [    3,     3,   640,   640], type =    f16, size =    7.031 MB
[  89/1680]   input_blocks.5.0.out_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  90/1680] input_blocks.5.0.out_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  91/1680]   input_blocks.5.0.out_layers.3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  92/1680] input_blocks.5.0.out_layers.3.weight - [    3,     3,   640,   640], type =    f16, size =    7.031 MB
[  93/1680]           input_blocks.5.1.norm.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  94/1680]         input_blocks.5.1.norm.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  95/1680]        input_blocks.5.1.proj_in.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  96/1680]      input_blocks.5.1.proj_in.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  97/1680]       input_blocks.5.1.proj_out.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[  98/1680]     input_blocks.5.1.proj_out.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[  99/1680] input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[ 100/1680] input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 101/1680] input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[ 102/1680] input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[ 103/1680] input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.78 MiB ->     0.42 MiB
[ 104/1680] input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB
[ 105/1680] input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 106/1680] input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[ 107/1680] input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[ 108/1680] input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q6_K .. size =     2.50 MiB ->     1.03 MiB
[ 109/1680] input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB
[ 110/1680] input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight - [  640,  5120,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 5120 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     6.25 MiB ->     2.34 MiB
[ 111/1680] input_blocks.5.1.transformer_blocks.0.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 112/1680] input_blocks.5.1.transformer_blocks.0.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 113/1680] input_blocks.5.1.transformer_blocks.0.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 114/1680] input_blocks.5.1.transformer_blocks.0.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 115/1680] input_blocks.5.1.transformer_blocks.0.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 116/1680] input_blocks.5.1.transformer_blocks.0.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 117/1680] input_blocks.5.1.transformer_blocks.0.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 118/1680] input_blocks.5.1.transformer_blocks.0.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 119/1680] input_blocks.5.1.transformer_blocks.1.attn1.to_k.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[ 120/1680] input_blocks.5.1.transformer_blocks.1.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 121/1680] input_blocks.5.1.transformer_blocks.1.attn1.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[ 122/1680] input_blocks.5.1.transformer_blocks.1.attn1.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[ 123/1680] input_blocks.5.1.transformer_blocks.1.attn1.to_v.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.78 MiB ->     0.42 MiB
[ 124/1680] input_blocks.5.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB
[ 125/1680] input_blocks.5.1.transformer_blocks.1.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 126/1680] input_blocks.5.1.transformer_blocks.1.attn2.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[ 127/1680] input_blocks.5.1.transformer_blocks.1.attn2.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[ 128/1680] input_blocks.5.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q6_K .. size =     2.50 MiB ->     1.03 MiB
[ 129/1680] input_blocks.5.1.transformer_blocks.1.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB
[ 130/1680] input_blocks.5.1.transformer_blocks.1.ff.net.0.proj.weight - [  640,  5120,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 5120 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     6.25 MiB ->     2.34 MiB
[ 131/1680] input_blocks.5.1.transformer_blocks.1.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 132/1680] input_blocks.5.1.transformer_blocks.1.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 133/1680] input_blocks.5.1.transformer_blocks.1.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 134/1680] input_blocks.5.1.transformer_blocks.1.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 135/1680] input_blocks.5.1.transformer_blocks.1.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 136/1680] input_blocks.5.1.transformer_blocks.1.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 137/1680] input_blocks.5.1.transformer_blocks.1.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 138/1680] input_blocks.5.1.transformer_blocks.1.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 139/1680]             input_blocks.6.0.op.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 140/1680]           input_blocks.6.0.op.weight - [    3,     3,   640,   640], type =    f16, size =    7.031 MB
[ 141/1680]   input_blocks.7.0.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 142/1680] input_blocks.7.0.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 143/1680]    input_blocks.7.0.in_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 144/1680]  input_blocks.7.0.in_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[ 145/1680]    input_blocks.7.0.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 146/1680]  input_blocks.7.0.in_layers.2.weight - [    3,     3,   640,  1280], type =    f16, size =   14.062 MB
[ 147/1680]   input_blocks.7.0.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 148/1680] input_blocks.7.0.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 149/1680]   input_blocks.7.0.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 150/1680] input_blocks.7.0.out_layers.3.weight - [    3,     3,  1280,  1280], type =    f16, size =   28.125 MB
[ 151/1680] input_blocks.7.0.skip_connection.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 152/1680] input_blocks.7.0.skip_connection.weight - [    1,     1,   640,  1280], type =    f16, size =    1.562 MB
[ 153/1680]           input_blocks.7.1.norm.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 154/1680]         input_blocks.7.1.norm.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 155/1680]        input_blocks.7.1.proj_in.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 156/1680]      input_blocks.7.1.proj_in.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 157/1680]       input_blocks.7.1.proj_out.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 158/1680]     input_blocks.7.1.proj_out.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 159/1680] input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 160/1680] input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 161/1680] input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 162/1680] input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 163/1680] input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 164/1680] input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 165/1680] input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 166/1680] input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 167/1680] input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 168/1680] input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 169/1680] input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 170/1680] input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 171/1680] input_blocks.7.1.transformer_blocks.0.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 172/1680] input_blocks.7.1.transformer_blocks.0.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 173/1680] input_blocks.7.1.transformer_blocks.0.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 174/1680] input_blocks.7.1.transformer_blocks.0.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 175/1680] input_blocks.7.1.transformer_blocks.0.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 176/1680] input_blocks.7.1.transformer_blocks.0.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 177/1680] input_blocks.7.1.transformer_blocks.0.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 178/1680] input_blocks.7.1.transformer_blocks.0.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 179/1680] input_blocks.7.1.transformer_blocks.1.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 180/1680] input_blocks.7.1.transformer_blocks.1.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 181/1680] input_blocks.7.1.transformer_blocks.1.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 182/1680] input_blocks.7.1.transformer_blocks.1.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 183/1680] input_blocks.7.1.transformer_blocks.1.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 184/1680] input_blocks.7.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 185/1680] input_blocks.7.1.transformer_blocks.1.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 186/1680] input_blocks.7.1.transformer_blocks.1.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 187/1680] input_blocks.7.1.transformer_blocks.1.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 188/1680] input_blocks.7.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 189/1680] input_blocks.7.1.transformer_blocks.1.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 190/1680] input_blocks.7.1.transformer_blocks.1.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 191/1680] input_blocks.7.1.transformer_blocks.1.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 192/1680] input_blocks.7.1.transformer_blocks.1.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 193/1680] input_blocks.7.1.transformer_blocks.1.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 194/1680] input_blocks.7.1.transformer_blocks.1.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 195/1680] input_blocks.7.1.transformer_blocks.1.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 196/1680] input_blocks.7.1.transformer_blocks.1.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 197/1680] input_blocks.7.1.transformer_blocks.1.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 198/1680] input_blocks.7.1.transformer_blocks.1.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 199/1680] input_blocks.7.1.transformer_blocks.2.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 200/1680] input_blocks.7.1.transformer_blocks.2.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 201/1680] input_blocks.7.1.transformer_blocks.2.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 202/1680] input_blocks.7.1.transformer_blocks.2.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 203/1680] input_blocks.7.1.transformer_blocks.2.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 204/1680] input_blocks.7.1.transformer_blocks.2.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 205/1680] input_blocks.7.1.transformer_blocks.2.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 206/1680] input_blocks.7.1.transformer_blocks.2.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 207/1680] input_blocks.7.1.transformer_blocks.2.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 208/1680] input_blocks.7.1.transformer_blocks.2.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 209/1680] input_blocks.7.1.transformer_blocks.2.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 210/1680] input_blocks.7.1.transformer_blocks.2.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 211/1680] input_blocks.7.1.transformer_blocks.2.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 212/1680] input_blocks.7.1.transformer_blocks.2.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 213/1680] input_blocks.7.1.transformer_blocks.2.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 214/1680] input_blocks.7.1.transformer_blocks.2.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 215/1680] input_blocks.7.1.transformer_blocks.2.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 216/1680] input_blocks.7.1.transformer_blocks.2.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 217/1680] input_blocks.7.1.transformer_blocks.2.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 218/1680] input_blocks.7.1.transformer_blocks.2.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 219/1680] input_blocks.7.1.transformer_blocks.3.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 220/1680] input_blocks.7.1.transformer_blocks.3.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 221/1680] input_blocks.7.1.transformer_blocks.3.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 222/1680] input_blocks.7.1.transformer_blocks.3.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 223/1680] input_blocks.7.1.transformer_blocks.3.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 224/1680] input_blocks.7.1.transformer_blocks.3.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 225/1680] input_blocks.7.1.transformer_blocks.3.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 226/1680] input_blocks.7.1.transformer_blocks.3.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 227/1680] input_blocks.7.1.transformer_blocks.3.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 228/1680] input_blocks.7.1.transformer_blocks.3.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 229/1680] input_blocks.7.1.transformer_blocks.3.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 230/1680] input_blocks.7.1.transformer_blocks.3.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 231/1680] input_blocks.7.1.transformer_blocks.3.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 232/1680] input_blocks.7.1.transformer_blocks.3.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 233/1680] input_blocks.7.1.transformer_blocks.3.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 234/1680] input_blocks.7.1.transformer_blocks.3.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 235/1680] input_blocks.7.1.transformer_blocks.3.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 236/1680] input_blocks.7.1.transformer_blocks.3.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 237/1680] input_blocks.7.1.transformer_blocks.3.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 238/1680] input_blocks.7.1.transformer_blocks.3.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 239/1680] input_blocks.7.1.transformer_blocks.4.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 240/1680] input_blocks.7.1.transformer_blocks.4.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 241/1680] input_blocks.7.1.transformer_blocks.4.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 242/1680] input_blocks.7.1.transformer_blocks.4.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 243/1680] input_blocks.7.1.transformer_blocks.4.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 244/1680] input_blocks.7.1.transformer_blocks.4.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 245/1680] input_blocks.7.1.transformer_blocks.4.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 246/1680] input_blocks.7.1.transformer_blocks.4.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 247/1680] input_blocks.7.1.transformer_blocks.4.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 248/1680] input_blocks.7.1.transformer_blocks.4.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 249/1680] input_blocks.7.1.transformer_blocks.4.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 250/1680] input_blocks.7.1.transformer_blocks.4.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 251/1680] input_blocks.7.1.transformer_blocks.4.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 252/1680] input_blocks.7.1.transformer_blocks.4.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 253/1680] input_blocks.7.1.transformer_blocks.4.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 254/1680] input_blocks.7.1.transformer_blocks.4.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 255/1680] input_blocks.7.1.transformer_blocks.4.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 256/1680] input_blocks.7.1.transformer_blocks.4.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 257/1680] input_blocks.7.1.transformer_blocks.4.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 258/1680] input_blocks.7.1.transformer_blocks.4.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 259/1680] input_blocks.7.1.transformer_blocks.5.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 260/1680] input_blocks.7.1.transformer_blocks.5.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 261/1680] input_blocks.7.1.transformer_blocks.5.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 262/1680] input_blocks.7.1.transformer_blocks.5.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 263/1680] input_blocks.7.1.transformer_blocks.5.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 264/1680] input_blocks.7.1.transformer_blocks.5.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 265/1680] input_blocks.7.1.transformer_blocks.5.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 266/1680] input_blocks.7.1.transformer_blocks.5.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 267/1680] input_blocks.7.1.transformer_blocks.5.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 268/1680] input_blocks.7.1.transformer_blocks.5.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 269/1680] input_blocks.7.1.transformer_blocks.5.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 270/1680] input_blocks.7.1.transformer_blocks.5.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 271/1680] input_blocks.7.1.transformer_blocks.5.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 272/1680] input_blocks.7.1.transformer_blocks.5.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 273/1680] input_blocks.7.1.transformer_blocks.5.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 274/1680] input_blocks.7.1.transformer_blocks.5.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 275/1680] input_blocks.7.1.transformer_blocks.5.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 276/1680] input_blocks.7.1.transformer_blocks.5.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 277/1680] input_blocks.7.1.transformer_blocks.5.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 278/1680] input_blocks.7.1.transformer_blocks.5.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 279/1680] input_blocks.7.1.transformer_blocks.6.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 280/1680] input_blocks.7.1.transformer_blocks.6.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 281/1680] input_blocks.7.1.transformer_blocks.6.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 282/1680] input_blocks.7.1.transformer_blocks.6.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 283/1680] input_blocks.7.1.transformer_blocks.6.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 284/1680] input_blocks.7.1.transformer_blocks.6.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 285/1680] input_blocks.7.1.transformer_blocks.6.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 286/1680] input_blocks.7.1.transformer_blocks.6.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 287/1680] input_blocks.7.1.transformer_blocks.6.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 288/1680] input_blocks.7.1.transformer_blocks.6.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 289/1680] input_blocks.7.1.transformer_blocks.6.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 290/1680] input_blocks.7.1.transformer_blocks.6.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 291/1680] input_blocks.7.1.transformer_blocks.6.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 292/1680] input_blocks.7.1.transformer_blocks.6.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 293/1680] input_blocks.7.1.transformer_blocks.6.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 294/1680] input_blocks.7.1.transformer_blocks.6.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 295/1680] input_blocks.7.1.transformer_blocks.6.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 296/1680] input_blocks.7.1.transformer_blocks.6.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 297/1680] input_blocks.7.1.transformer_blocks.6.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 298/1680] input_blocks.7.1.transformer_blocks.6.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 299/1680] input_blocks.7.1.transformer_blocks.7.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 300/1680] input_blocks.7.1.transformer_blocks.7.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 301/1680] input_blocks.7.1.transformer_blocks.7.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 302/1680] input_blocks.7.1.transformer_blocks.7.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 303/1680] input_blocks.7.1.transformer_blocks.7.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 304/1680] input_blocks.7.1.transformer_blocks.7.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 305/1680] input_blocks.7.1.transformer_blocks.7.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 306/1680] input_blocks.7.1.transformer_blocks.7.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 307/1680] input_blocks.7.1.transformer_blocks.7.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 308/1680] input_blocks.7.1.transformer_blocks.7.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 309/1680] input_blocks.7.1.transformer_blocks.7.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 310/1680] input_blocks.7.1.transformer_blocks.7.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 311/1680] input_blocks.7.1.transformer_blocks.7.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 312/1680] input_blocks.7.1.transformer_blocks.7.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 313/1680] input_blocks.7.1.transformer_blocks.7.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 314/1680] input_blocks.7.1.transformer_blocks.7.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 315/1680] input_blocks.7.1.transformer_blocks.7.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 316/1680] input_blocks.7.1.transformer_blocks.7.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 317/1680] input_blocks.7.1.transformer_blocks.7.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 318/1680] input_blocks.7.1.transformer_blocks.7.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 319/1680] input_blocks.7.1.transformer_blocks.8.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 320/1680] input_blocks.7.1.transformer_blocks.8.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 321/1680] input_blocks.7.1.transformer_blocks.8.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 322/1680] input_blocks.7.1.transformer_blocks.8.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 323/1680] input_blocks.7.1.transformer_blocks.8.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 324/1680] input_blocks.7.1.transformer_blocks.8.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 325/1680] input_blocks.7.1.transformer_blocks.8.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 326/1680] input_blocks.7.1.transformer_blocks.8.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 327/1680] input_blocks.7.1.transformer_blocks.8.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 328/1680] input_blocks.7.1.transformer_blocks.8.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 329/1680] input_blocks.7.1.transformer_blocks.8.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 330/1680] input_blocks.7.1.transformer_blocks.8.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 331/1680] input_blocks.7.1.transformer_blocks.8.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 332/1680] input_blocks.7.1.transformer_blocks.8.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 333/1680] input_blocks.7.1.transformer_blocks.8.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 334/1680] input_blocks.7.1.transformer_blocks.8.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 335/1680] input_blocks.7.1.transformer_blocks.8.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 336/1680] input_blocks.7.1.transformer_blocks.8.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 337/1680] input_blocks.7.1.transformer_blocks.8.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 338/1680] input_blocks.7.1.transformer_blocks.8.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 339/1680] input_blocks.7.1.transformer_blocks.9.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 340/1680] input_blocks.7.1.transformer_blocks.9.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 341/1680] input_blocks.7.1.transformer_blocks.9.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 342/1680] input_blocks.7.1.transformer_blocks.9.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 343/1680] input_blocks.7.1.transformer_blocks.9.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 344/1680] input_blocks.7.1.transformer_blocks.9.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 345/1680] input_blocks.7.1.transformer_blocks.9.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 346/1680] input_blocks.7.1.transformer_blocks.9.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 347/1680] input_blocks.7.1.transformer_blocks.9.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 348/1680] input_blocks.7.1.transformer_blocks.9.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 349/1680] input_blocks.7.1.transformer_blocks.9.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 350/1680] input_blocks.7.1.transformer_blocks.9.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 351/1680] input_blocks.7.1.transformer_blocks.9.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 352/1680] input_blocks.7.1.transformer_blocks.9.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 353/1680] input_blocks.7.1.transformer_blocks.9.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 354/1680] input_blocks.7.1.transformer_blocks.9.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 355/1680] input_blocks.7.1.transformer_blocks.9.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 356/1680] input_blocks.7.1.transformer_blocks.9.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 357/1680] input_blocks.7.1.transformer_blocks.9.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 358/1680] input_blocks.7.1.transformer_blocks.9.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 359/1680]   input_blocks.8.0.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 360/1680] input_blocks.8.0.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 361/1680]    input_blocks.8.0.in_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 362/1680]  input_blocks.8.0.in_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 363/1680]    input_blocks.8.0.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 364/1680]  input_blocks.8.0.in_layers.2.weight - [    3,     3,  1280,  1280], type =    f16, size =   28.125 MB
[ 365/1680]   input_blocks.8.0.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 366/1680] input_blocks.8.0.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 367/1680]   input_blocks.8.0.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 368/1680] input_blocks.8.0.out_layers.3.weight - [    3,     3,  1280,  1280], type =    f16, size =   28.125 MB
[ 369/1680]           input_blocks.8.1.norm.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 370/1680]         input_blocks.8.1.norm.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 371/1680]        input_blocks.8.1.proj_in.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 372/1680]      input_blocks.8.1.proj_in.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 373/1680]       input_blocks.8.1.proj_out.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 374/1680]     input_blocks.8.1.proj_out.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 375/1680] input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 376/1680] input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 377/1680] input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 378/1680] input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 379/1680] input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 380/1680] input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 381/1680] input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 382/1680] input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 383/1680] input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 384/1680] input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 385/1680] input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 386/1680] input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 387/1680] input_blocks.8.1.transformer_blocks.0.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 388/1680] input_blocks.8.1.transformer_blocks.0.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 389/1680] input_blocks.8.1.transformer_blocks.0.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 390/1680] input_blocks.8.1.transformer_blocks.0.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 391/1680] input_blocks.8.1.transformer_blocks.0.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 392/1680] input_blocks.8.1.transformer_blocks.0.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 393/1680] input_blocks.8.1.transformer_blocks.0.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 394/1680] input_blocks.8.1.transformer_blocks.0.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 395/1680] input_blocks.8.1.transformer_blocks.1.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 396/1680] input_blocks.8.1.transformer_blocks.1.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 397/1680] input_blocks.8.1.transformer_blocks.1.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 398/1680] input_blocks.8.1.transformer_blocks.1.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 399/1680] input_blocks.8.1.transformer_blocks.1.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 400/1680] input_blocks.8.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 401/1680] input_blocks.8.1.transformer_blocks.1.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 402/1680] input_blocks.8.1.transformer_blocks.1.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 403/1680] input_blocks.8.1.transformer_blocks.1.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 404/1680] input_blocks.8.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 405/1680] input_blocks.8.1.transformer_blocks.1.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 406/1680] input_blocks.8.1.transformer_blocks.1.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 407/1680] input_blocks.8.1.transformer_blocks.1.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 408/1680] input_blocks.8.1.transformer_blocks.1.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 409/1680] input_blocks.8.1.transformer_blocks.1.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 410/1680] input_blocks.8.1.transformer_blocks.1.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 411/1680] input_blocks.8.1.transformer_blocks.1.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 412/1680] input_blocks.8.1.transformer_blocks.1.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 413/1680] input_blocks.8.1.transformer_blocks.1.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 414/1680] input_blocks.8.1.transformer_blocks.1.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 415/1680] input_blocks.8.1.transformer_blocks.2.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 416/1680] input_blocks.8.1.transformer_blocks.2.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 417/1680] input_blocks.8.1.transformer_blocks.2.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 418/1680] input_blocks.8.1.transformer_blocks.2.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 419/1680] input_blocks.8.1.transformer_blocks.2.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 420/1680] input_blocks.8.1.transformer_blocks.2.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 421/1680] input_blocks.8.1.transformer_blocks.2.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 422/1680] input_blocks.8.1.transformer_blocks.2.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 423/1680] input_blocks.8.1.transformer_blocks.2.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 424/1680] input_blocks.8.1.transformer_blocks.2.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 425/1680] input_blocks.8.1.transformer_blocks.2.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 426/1680] input_blocks.8.1.transformer_blocks.2.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 427/1680] input_blocks.8.1.transformer_blocks.2.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 428/1680] input_blocks.8.1.transformer_blocks.2.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 429/1680] input_blocks.8.1.transformer_blocks.2.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 430/1680] input_blocks.8.1.transformer_blocks.2.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 431/1680] input_blocks.8.1.transformer_blocks.2.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 432/1680] input_blocks.8.1.transformer_blocks.2.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 433/1680] input_blocks.8.1.transformer_blocks.2.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 434/1680] input_blocks.8.1.transformer_blocks.2.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 435/1680] input_blocks.8.1.transformer_blocks.3.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 436/1680] input_blocks.8.1.transformer_blocks.3.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 437/1680] input_blocks.8.1.transformer_blocks.3.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 438/1680] input_blocks.8.1.transformer_blocks.3.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 439/1680] input_blocks.8.1.transformer_blocks.3.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 440/1680] input_blocks.8.1.transformer_blocks.3.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 441/1680] input_blocks.8.1.transformer_blocks.3.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 442/1680] input_blocks.8.1.transformer_blocks.3.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 443/1680] input_blocks.8.1.transformer_blocks.3.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 444/1680] input_blocks.8.1.transformer_blocks.3.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 445/1680] input_blocks.8.1.transformer_blocks.3.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 446/1680] input_blocks.8.1.transformer_blocks.3.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 447/1680] input_blocks.8.1.transformer_blocks.3.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 448/1680] input_blocks.8.1.transformer_blocks.3.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 449/1680] input_blocks.8.1.transformer_blocks.3.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 450/1680] input_blocks.8.1.transformer_blocks.3.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 451/1680] input_blocks.8.1.transformer_blocks.3.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 452/1680] input_blocks.8.1.transformer_blocks.3.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 453/1680] input_blocks.8.1.transformer_blocks.3.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 454/1680] input_blocks.8.1.transformer_blocks.3.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 455/1680] input_blocks.8.1.transformer_blocks.4.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 456/1680] input_blocks.8.1.transformer_blocks.4.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 457/1680] input_blocks.8.1.transformer_blocks.4.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 458/1680] input_blocks.8.1.transformer_blocks.4.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 459/1680] input_blocks.8.1.transformer_blocks.4.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 460/1680] input_blocks.8.1.transformer_blocks.4.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 461/1680] input_blocks.8.1.transformer_blocks.4.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 462/1680] input_blocks.8.1.transformer_blocks.4.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 463/1680] input_blocks.8.1.transformer_blocks.4.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 464/1680] input_blocks.8.1.transformer_blocks.4.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 465/1680] input_blocks.8.1.transformer_blocks.4.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 466/1680] input_blocks.8.1.transformer_blocks.4.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 467/1680] input_blocks.8.1.transformer_blocks.4.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 468/1680] input_blocks.8.1.transformer_blocks.4.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 469/1680] input_blocks.8.1.transformer_blocks.4.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 470/1680] input_blocks.8.1.transformer_blocks.4.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 471/1680] input_blocks.8.1.transformer_blocks.4.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 472/1680] input_blocks.8.1.transformer_blocks.4.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 473/1680] input_blocks.8.1.transformer_blocks.4.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 474/1680] input_blocks.8.1.transformer_blocks.4.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 475/1680] input_blocks.8.1.transformer_blocks.5.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 476/1680] input_blocks.8.1.transformer_blocks.5.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 477/1680] input_blocks.8.1.transformer_blocks.5.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 478/1680] input_blocks.8.1.transformer_blocks.5.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 479/1680] input_blocks.8.1.transformer_blocks.5.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 480/1680] input_blocks.8.1.transformer_blocks.5.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 481/1680] input_blocks.8.1.transformer_blocks.5.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 482/1680] input_blocks.8.1.transformer_blocks.5.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 483/1680] input_blocks.8.1.transformer_blocks.5.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 484/1680] input_blocks.8.1.transformer_blocks.5.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 485/1680] input_blocks.8.1.transformer_blocks.5.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 486/1680] input_blocks.8.1.transformer_blocks.5.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 487/1680] input_blocks.8.1.transformer_blocks.5.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 488/1680] input_blocks.8.1.transformer_blocks.5.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 489/1680] input_blocks.8.1.transformer_blocks.5.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 490/1680] input_blocks.8.1.transformer_blocks.5.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 491/1680] input_blocks.8.1.transformer_blocks.5.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 492/1680] input_blocks.8.1.transformer_blocks.5.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 493/1680] input_blocks.8.1.transformer_blocks.5.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 494/1680] input_blocks.8.1.transformer_blocks.5.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 495/1680] input_blocks.8.1.transformer_blocks.6.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 496/1680] input_blocks.8.1.transformer_blocks.6.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 497/1680] input_blocks.8.1.transformer_blocks.6.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 498/1680] input_blocks.8.1.transformer_blocks.6.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 499/1680] input_blocks.8.1.transformer_blocks.6.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 500/1680] input_blocks.8.1.transformer_blocks.6.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 501/1680] input_blocks.8.1.transformer_blocks.6.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 502/1680] input_blocks.8.1.transformer_blocks.6.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 503/1680] input_blocks.8.1.transformer_blocks.6.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 504/1680] input_blocks.8.1.transformer_blocks.6.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 505/1680] input_blocks.8.1.transformer_blocks.6.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 506/1680] input_blocks.8.1.transformer_blocks.6.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 507/1680] input_blocks.8.1.transformer_blocks.6.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 508/1680] input_blocks.8.1.transformer_blocks.6.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 509/1680] input_blocks.8.1.transformer_blocks.6.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 510/1680] input_blocks.8.1.transformer_blocks.6.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 511/1680] input_blocks.8.1.transformer_blocks.6.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 512/1680] input_blocks.8.1.transformer_blocks.6.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 513/1680] input_blocks.8.1.transformer_blocks.6.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 514/1680] input_blocks.8.1.transformer_blocks.6.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 515/1680] input_blocks.8.1.transformer_blocks.7.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 516/1680] input_blocks.8.1.transformer_blocks.7.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 517/1680] input_blocks.8.1.transformer_blocks.7.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 518/1680] input_blocks.8.1.transformer_blocks.7.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 519/1680] input_blocks.8.1.transformer_blocks.7.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 520/1680] input_blocks.8.1.transformer_blocks.7.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 521/1680] input_blocks.8.1.transformer_blocks.7.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 522/1680] input_blocks.8.1.transformer_blocks.7.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 523/1680] input_blocks.8.1.transformer_blocks.7.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 524/1680] input_blocks.8.1.transformer_blocks.7.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 525/1680] input_blocks.8.1.transformer_blocks.7.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 526/1680] input_blocks.8.1.transformer_blocks.7.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 527/1680] input_blocks.8.1.transformer_blocks.7.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 528/1680] input_blocks.8.1.transformer_blocks.7.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 529/1680] input_blocks.8.1.transformer_blocks.7.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 530/1680] input_blocks.8.1.transformer_blocks.7.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 531/1680] input_blocks.8.1.transformer_blocks.7.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 532/1680] input_blocks.8.1.transformer_blocks.7.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 533/1680] input_blocks.8.1.transformer_blocks.7.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 534/1680] input_blocks.8.1.transformer_blocks.7.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 535/1680] input_blocks.8.1.transformer_blocks.8.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 536/1680] input_blocks.8.1.transformer_blocks.8.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 537/1680] input_blocks.8.1.transformer_blocks.8.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 538/1680] input_blocks.8.1.transformer_blocks.8.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 539/1680] input_blocks.8.1.transformer_blocks.8.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 540/1680] input_blocks.8.1.transformer_blocks.8.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 541/1680] input_blocks.8.1.transformer_blocks.8.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 542/1680] input_blocks.8.1.transformer_blocks.8.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 543/1680] input_blocks.8.1.transformer_blocks.8.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 544/1680] input_blocks.8.1.transformer_blocks.8.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 545/1680] input_blocks.8.1.transformer_blocks.8.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 546/1680] input_blocks.8.1.transformer_blocks.8.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 547/1680] input_blocks.8.1.transformer_blocks.8.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 548/1680] input_blocks.8.1.transformer_blocks.8.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 549/1680] input_blocks.8.1.transformer_blocks.8.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 550/1680] input_blocks.8.1.transformer_blocks.8.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 551/1680] input_blocks.8.1.transformer_blocks.8.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 552/1680] input_blocks.8.1.transformer_blocks.8.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 553/1680] input_blocks.8.1.transformer_blocks.8.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 554/1680] input_blocks.8.1.transformer_blocks.8.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 555/1680] input_blocks.8.1.transformer_blocks.9.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 556/1680] input_blocks.8.1.transformer_blocks.9.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 557/1680] input_blocks.8.1.transformer_blocks.9.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 558/1680] input_blocks.8.1.transformer_blocks.9.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 559/1680] input_blocks.8.1.transformer_blocks.9.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 560/1680] input_blocks.8.1.transformer_blocks.9.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 561/1680] input_blocks.8.1.transformer_blocks.9.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 562/1680] input_blocks.8.1.transformer_blocks.9.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 563/1680] input_blocks.8.1.transformer_blocks.9.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 564/1680] input_blocks.8.1.transformer_blocks.9.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 565/1680] input_blocks.8.1.transformer_blocks.9.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 566/1680] input_blocks.8.1.transformer_blocks.9.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 567/1680] input_blocks.8.1.transformer_blocks.9.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 568/1680] input_blocks.8.1.transformer_blocks.9.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 569/1680] input_blocks.8.1.transformer_blocks.9.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 570/1680] input_blocks.8.1.transformer_blocks.9.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 571/1680] input_blocks.8.1.transformer_blocks.9.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 572/1680] input_blocks.8.1.transformer_blocks.9.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 573/1680] input_blocks.8.1.transformer_blocks.9.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 574/1680] input_blocks.8.1.transformer_blocks.9.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 575/1680]                   label_emb.0.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 576/1680]                 label_emb.0.0.weight - [ 2816,  1280,     1,     1], type =    f16, size =    6.875 MB
[ 577/1680]                   label_emb.0.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 578/1680]                 label_emb.0.2.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB
[ 579/1680]     middle_block.0.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 580/1680]   middle_block.0.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 581/1680]      middle_block.0.in_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 582/1680]    middle_block.0.in_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 583/1680]      middle_block.0.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 584/1680]    middle_block.0.in_layers.2.weight - [    3,     3,  1280,  1280], type =    f16, size =   28.125 MB
[ 585/1680]     middle_block.0.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 586/1680]   middle_block.0.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 587/1680]     middle_block.0.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 588/1680]   middle_block.0.out_layers.3.weight - [    3,     3,  1280,  1280], type =    f16, size =   28.125 MB
[ 589/1680]             middle_block.1.norm.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 590/1680]           middle_block.1.norm.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 591/1680]          middle_block.1.proj_in.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 592/1680]        middle_block.1.proj_in.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 593/1680]         middle_block.1.proj_out.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 594/1680]       middle_block.1.proj_out.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 595/1680] middle_block.1.transformer_blocks.0.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 596/1680] middle_block.1.transformer_blocks.0.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 597/1680] middle_block.1.transformer_blocks.0.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 598/1680] middle_block.1.transformer_blocks.0.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 599/1680] middle_block.1.transformer_blocks.0.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 600/1680] middle_block.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 601/1680] middle_block.1.transformer_blocks.0.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 602/1680] middle_block.1.transformer_blocks.0.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 603/1680] middle_block.1.transformer_blocks.0.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 604/1680] middle_block.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 605/1680] middle_block.1.transformer_blocks.0.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 606/1680] middle_block.1.transformer_blocks.0.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 607/1680] middle_block.1.transformer_blocks.0.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 608/1680] middle_block.1.transformer_blocks.0.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 609/1680] middle_block.1.transformer_blocks.0.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 610/1680] middle_block.1.transformer_blocks.0.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 611/1680] middle_block.1.transformer_blocks.0.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 612/1680] middle_block.1.transformer_blocks.0.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 613/1680] middle_block.1.transformer_blocks.0.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 614/1680] middle_block.1.transformer_blocks.0.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 615/1680] middle_block.1.transformer_blocks.1.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 616/1680] middle_block.1.transformer_blocks.1.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 617/1680] middle_block.1.transformer_blocks.1.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 618/1680] middle_block.1.transformer_blocks.1.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 619/1680] middle_block.1.transformer_blocks.1.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 620/1680] middle_block.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 621/1680] middle_block.1.transformer_blocks.1.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 622/1680] middle_block.1.transformer_blocks.1.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 623/1680] middle_block.1.transformer_blocks.1.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 624/1680] middle_block.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 625/1680] middle_block.1.transformer_blocks.1.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 626/1680] middle_block.1.transformer_blocks.1.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 627/1680] middle_block.1.transformer_blocks.1.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 628/1680] middle_block.1.transformer_blocks.1.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 629/1680] middle_block.1.transformer_blocks.1.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 630/1680] middle_block.1.transformer_blocks.1.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 631/1680] middle_block.1.transformer_blocks.1.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 632/1680] middle_block.1.transformer_blocks.1.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 633/1680] middle_block.1.transformer_blocks.1.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 634/1680] middle_block.1.transformer_blocks.1.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 635/1680] middle_block.1.transformer_blocks.2.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 636/1680] middle_block.1.transformer_blocks.2.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 637/1680] middle_block.1.transformer_blocks.2.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 638/1680] middle_block.1.transformer_blocks.2.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 639/1680] middle_block.1.transformer_blocks.2.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 640/1680] middle_block.1.transformer_blocks.2.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 641/1680] middle_block.1.transformer_blocks.2.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 642/1680] middle_block.1.transformer_blocks.2.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 643/1680] middle_block.1.transformer_blocks.2.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 644/1680] middle_block.1.transformer_blocks.2.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 645/1680] middle_block.1.transformer_blocks.2.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 646/1680] middle_block.1.transformer_blocks.2.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 647/1680] middle_block.1.transformer_blocks.2.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 648/1680] middle_block.1.transformer_blocks.2.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 649/1680] middle_block.1.transformer_blocks.2.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 650/1680] middle_block.1.transformer_blocks.2.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 651/1680] middle_block.1.transformer_blocks.2.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 652/1680] middle_block.1.transformer_blocks.2.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 653/1680] middle_block.1.transformer_blocks.2.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 654/1680] middle_block.1.transformer_blocks.2.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 655/1680] middle_block.1.transformer_blocks.3.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 656/1680] middle_block.1.transformer_blocks.3.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 657/1680] middle_block.1.transformer_blocks.3.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 658/1680] middle_block.1.transformer_blocks.3.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 659/1680] middle_block.1.transformer_blocks.3.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 660/1680] middle_block.1.transformer_blocks.3.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 661/1680] middle_block.1.transformer_blocks.3.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 662/1680] middle_block.1.transformer_blocks.3.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 663/1680] middle_block.1.transformer_blocks.3.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 664/1680] middle_block.1.transformer_blocks.3.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 665/1680] middle_block.1.transformer_blocks.3.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 666/1680] middle_block.1.transformer_blocks.3.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 667/1680] middle_block.1.transformer_blocks.3.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 668/1680] middle_block.1.transformer_blocks.3.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 669/1680] middle_block.1.transformer_blocks.3.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 670/1680] middle_block.1.transformer_blocks.3.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 671/1680] middle_block.1.transformer_blocks.3.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 672/1680] middle_block.1.transformer_blocks.3.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 673/1680] middle_block.1.transformer_blocks.3.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 674/1680] middle_block.1.transformer_blocks.3.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 675/1680] middle_block.1.transformer_blocks.4.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 676/1680] middle_block.1.transformer_blocks.4.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 677/1680] middle_block.1.transformer_blocks.4.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 678/1680] middle_block.1.transformer_blocks.4.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 679/1680] middle_block.1.transformer_blocks.4.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 680/1680] middle_block.1.transformer_blocks.4.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 681/1680] middle_block.1.transformer_blocks.4.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 682/1680] middle_block.1.transformer_blocks.4.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 683/1680] middle_block.1.transformer_blocks.4.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 684/1680] middle_block.1.transformer_blocks.4.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 685/1680] middle_block.1.transformer_blocks.4.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 686/1680] middle_block.1.transformer_blocks.4.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 687/1680] middle_block.1.transformer_blocks.4.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 688/1680] middle_block.1.transformer_blocks.4.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 689/1680] middle_block.1.transformer_blocks.4.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 690/1680] middle_block.1.transformer_blocks.4.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 691/1680] middle_block.1.transformer_blocks.4.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 692/1680] middle_block.1.transformer_blocks.4.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 693/1680] middle_block.1.transformer_blocks.4.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 694/1680] middle_block.1.transformer_blocks.4.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 695/1680] middle_block.1.transformer_blocks.5.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 696/1680] middle_block.1.transformer_blocks.5.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 697/1680] middle_block.1.transformer_blocks.5.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 698/1680] middle_block.1.transformer_blocks.5.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 699/1680] middle_block.1.transformer_blocks.5.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 700/1680] middle_block.1.transformer_blocks.5.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 701/1680] middle_block.1.transformer_blocks.5.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 702/1680] middle_block.1.transformer_blocks.5.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 703/1680] middle_block.1.transformer_blocks.5.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 704/1680] middle_block.1.transformer_blocks.5.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 705/1680] middle_block.1.transformer_blocks.5.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 706/1680] middle_block.1.transformer_blocks.5.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 707/1680] middle_block.1.transformer_blocks.5.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 708/1680] middle_block.1.transformer_blocks.5.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 709/1680] middle_block.1.transformer_blocks.5.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 710/1680] middle_block.1.transformer_blocks.5.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 711/1680] middle_block.1.transformer_blocks.5.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 712/1680] middle_block.1.transformer_blocks.5.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 713/1680] middle_block.1.transformer_blocks.5.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 714/1680] middle_block.1.transformer_blocks.5.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 715/1680] middle_block.1.transformer_blocks.6.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 716/1680] middle_block.1.transformer_blocks.6.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 717/1680] middle_block.1.transformer_blocks.6.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 718/1680] middle_block.1.transformer_blocks.6.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 719/1680] middle_block.1.transformer_blocks.6.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 720/1680] middle_block.1.transformer_blocks.6.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 721/1680] middle_block.1.transformer_blocks.6.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 722/1680] middle_block.1.transformer_blocks.6.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 723/1680] middle_block.1.transformer_blocks.6.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 724/1680] middle_block.1.transformer_blocks.6.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 725/1680] middle_block.1.transformer_blocks.6.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 726/1680] middle_block.1.transformer_blocks.6.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 727/1680] middle_block.1.transformer_blocks.6.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 728/1680] middle_block.1.transformer_blocks.6.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 729/1680] middle_block.1.transformer_blocks.6.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 730/1680] middle_block.1.transformer_blocks.6.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 731/1680] middle_block.1.transformer_blocks.6.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 732/1680] middle_block.1.transformer_blocks.6.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 733/1680] middle_block.1.transformer_blocks.6.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 734/1680] middle_block.1.transformer_blocks.6.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 735/1680] middle_block.1.transformer_blocks.7.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 736/1680] middle_block.1.transformer_blocks.7.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 737/1680] middle_block.1.transformer_blocks.7.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 738/1680] middle_block.1.transformer_blocks.7.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 739/1680] middle_block.1.transformer_blocks.7.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 740/1680] middle_block.1.transformer_blocks.7.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 741/1680] middle_block.1.transformer_blocks.7.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 742/1680] middle_block.1.transformer_blocks.7.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 743/1680] middle_block.1.transformer_blocks.7.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 744/1680] middle_block.1.transformer_blocks.7.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 745/1680] middle_block.1.transformer_blocks.7.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 746/1680] middle_block.1.transformer_blocks.7.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 747/1680] middle_block.1.transformer_blocks.7.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 748/1680] middle_block.1.transformer_blocks.7.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 749/1680] middle_block.1.transformer_blocks.7.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 750/1680] middle_block.1.transformer_blocks.7.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 751/1680] middle_block.1.transformer_blocks.7.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 752/1680] middle_block.1.transformer_blocks.7.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 753/1680] middle_block.1.transformer_blocks.7.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 754/1680] middle_block.1.transformer_blocks.7.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 755/1680] middle_block.1.transformer_blocks.8.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 756/1680] middle_block.1.transformer_blocks.8.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 757/1680] middle_block.1.transformer_blocks.8.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 758/1680] middle_block.1.transformer_blocks.8.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 759/1680] middle_block.1.transformer_blocks.8.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 760/1680] middle_block.1.transformer_blocks.8.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 761/1680] middle_block.1.transformer_blocks.8.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 762/1680] middle_block.1.transformer_blocks.8.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 763/1680] middle_block.1.transformer_blocks.8.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 764/1680] middle_block.1.transformer_blocks.8.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 765/1680] middle_block.1.transformer_blocks.8.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 766/1680] middle_block.1.transformer_blocks.8.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 767/1680] middle_block.1.transformer_blocks.8.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 768/1680] middle_block.1.transformer_blocks.8.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 769/1680] middle_block.1.transformer_blocks.8.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 770/1680] middle_block.1.transformer_blocks.8.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 771/1680] middle_block.1.transformer_blocks.8.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 772/1680] middle_block.1.transformer_blocks.8.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 773/1680] middle_block.1.transformer_blocks.8.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 774/1680] middle_block.1.transformer_blocks.8.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 775/1680] middle_block.1.transformer_blocks.9.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 776/1680] middle_block.1.transformer_blocks.9.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 777/1680] middle_block.1.transformer_blocks.9.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 778/1680] middle_block.1.transformer_blocks.9.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 779/1680] middle_block.1.transformer_blocks.9.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 780/1680] middle_block.1.transformer_blocks.9.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 781/1680] middle_block.1.transformer_blocks.9.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 782/1680] middle_block.1.transformer_blocks.9.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 783/1680] middle_block.1.transformer_blocks.9.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 784/1680] middle_block.1.transformer_blocks.9.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 785/1680] middle_block.1.transformer_blocks.9.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 786/1680] middle_block.1.transformer_blocks.9.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 787/1680] middle_block.1.transformer_blocks.9.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 788/1680] middle_block.1.transformer_blocks.9.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 789/1680] middle_block.1.transformer_blocks.9.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 790/1680] middle_block.1.transformer_blocks.9.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 791/1680] middle_block.1.transformer_blocks.9.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 792/1680] middle_block.1.transformer_blocks.9.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 793/1680] middle_block.1.transformer_blocks.9.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 794/1680] middle_block.1.transformer_blocks.9.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 795/1680]     middle_block.2.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 796/1680]   middle_block.2.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 797/1680]      middle_block.2.in_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 798/1680]    middle_block.2.in_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 799/1680]      middle_block.2.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 800/1680]    middle_block.2.in_layers.2.weight - [    3,     3,  1280,  1280], type =    f16, size =   28.125 MB
[ 801/1680]     middle_block.2.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 802/1680]   middle_block.2.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 803/1680]     middle_block.2.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 804/1680]   middle_block.2.out_layers.3.weight - [    3,     3,  1280,  1280], type =    f16, size =   28.125 MB
[ 805/1680]                           out.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[ 806/1680]                         out.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[ 807/1680]                           out.2.bias - [    4,     1,     1,     1], type =    f16, size =    0.000 MB
[ 808/1680]                         out.2.weight - [    3,     3,   320,     4], type =    f16, size =    0.022 MB
[ 809/1680]  output_blocks.0.0.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 810/1680] output_blocks.0.0.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 811/1680]   output_blocks.0.0.in_layers.0.bias - [ 2560,     1,     1,     1], type =    f16, size =    0.005 MB
[ 812/1680] output_blocks.0.0.in_layers.0.weight - [ 2560,     1,     1,     1], type =    f16, size =    0.005 MB
[ 813/1680]   output_blocks.0.0.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 814/1680] output_blocks.0.0.in_layers.2.weight - [    3,     3,  2560,  1280], type =    f16, size =   56.250 MB
[ 815/1680]  output_blocks.0.0.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 816/1680] output_blocks.0.0.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 817/1680]  output_blocks.0.0.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 818/1680] output_blocks.0.0.out_layers.3.weight - [    3,     3,  1280,  1280], type =    f16, size =   28.125 MB
[ 819/1680] output_blocks.0.0.skip_connection.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 820/1680] output_blocks.0.0.skip_connection.weight - [    1,     1,  2560,  1280], type =    f16, size =    6.250 MB
[ 821/1680]          output_blocks.0.1.norm.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 822/1680]        output_blocks.0.1.norm.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 823/1680]       output_blocks.0.1.proj_in.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 824/1680]     output_blocks.0.1.proj_in.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 825/1680]      output_blocks.0.1.proj_out.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 826/1680]    output_blocks.0.1.proj_out.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 827/1680] output_blocks.0.1.transformer_blocks.0.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 828/1680] output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 829/1680] output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 830/1680] output_blocks.0.1.transformer_blocks.0.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 831/1680] output_blocks.0.1.transformer_blocks.0.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 832/1680] output_blocks.0.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 833/1680] output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 834/1680] output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 835/1680] output_blocks.0.1.transformer_blocks.0.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 836/1680] output_blocks.0.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 837/1680] output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 838/1680] output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 839/1680] output_blocks.0.1.transformer_blocks.0.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 840/1680] output_blocks.0.1.transformer_blocks.0.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 841/1680] output_blocks.0.1.transformer_blocks.0.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 842/1680] output_blocks.0.1.transformer_blocks.0.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 843/1680] output_blocks.0.1.transformer_blocks.0.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 844/1680] output_blocks.0.1.transformer_blocks.0.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 845/1680] output_blocks.0.1.transformer_blocks.0.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 846/1680] output_blocks.0.1.transformer_blocks.0.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 847/1680] output_blocks.0.1.transformer_blocks.1.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 848/1680] output_blocks.0.1.transformer_blocks.1.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 849/1680] output_blocks.0.1.transformer_blocks.1.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 850/1680] output_blocks.0.1.transformer_blocks.1.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 851/1680] output_blocks.0.1.transformer_blocks.1.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 852/1680] output_blocks.0.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 853/1680] output_blocks.0.1.transformer_blocks.1.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 854/1680] output_blocks.0.1.transformer_blocks.1.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 855/1680] output_blocks.0.1.transformer_blocks.1.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 856/1680] output_blocks.0.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 857/1680] output_blocks.0.1.transformer_blocks.1.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 858/1680] output_blocks.0.1.transformer_blocks.1.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 859/1680] output_blocks.0.1.transformer_blocks.1.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 860/1680] output_blocks.0.1.transformer_blocks.1.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 861/1680] output_blocks.0.1.transformer_blocks.1.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 862/1680] output_blocks.0.1.transformer_blocks.1.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 863/1680] output_blocks.0.1.transformer_blocks.1.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 864/1680] output_blocks.0.1.transformer_blocks.1.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 865/1680] output_blocks.0.1.transformer_blocks.1.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 866/1680] output_blocks.0.1.transformer_blocks.1.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 867/1680] output_blocks.0.1.transformer_blocks.2.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 868/1680] output_blocks.0.1.transformer_blocks.2.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 869/1680] output_blocks.0.1.transformer_blocks.2.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 870/1680] output_blocks.0.1.transformer_blocks.2.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 871/1680] output_blocks.0.1.transformer_blocks.2.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 872/1680] output_blocks.0.1.transformer_blocks.2.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 873/1680] output_blocks.0.1.transformer_blocks.2.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 874/1680] output_blocks.0.1.transformer_blocks.2.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 875/1680] output_blocks.0.1.transformer_blocks.2.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 876/1680] output_blocks.0.1.transformer_blocks.2.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 877/1680] output_blocks.0.1.transformer_blocks.2.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 878/1680] output_blocks.0.1.transformer_blocks.2.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 879/1680] output_blocks.0.1.transformer_blocks.2.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 880/1680] output_blocks.0.1.transformer_blocks.2.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 881/1680] output_blocks.0.1.transformer_blocks.2.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 882/1680] output_blocks.0.1.transformer_blocks.2.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 883/1680] output_blocks.0.1.transformer_blocks.2.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 884/1680] output_blocks.0.1.transformer_blocks.2.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 885/1680] output_blocks.0.1.transformer_blocks.2.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 886/1680] output_blocks.0.1.transformer_blocks.2.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 887/1680] output_blocks.0.1.transformer_blocks.3.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 888/1680] output_blocks.0.1.transformer_blocks.3.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 889/1680] output_blocks.0.1.transformer_blocks.3.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 890/1680] output_blocks.0.1.transformer_blocks.3.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 891/1680] output_blocks.0.1.transformer_blocks.3.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 892/1680] output_blocks.0.1.transformer_blocks.3.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 893/1680] output_blocks.0.1.transformer_blocks.3.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 894/1680] output_blocks.0.1.transformer_blocks.3.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 895/1680] output_blocks.0.1.transformer_blocks.3.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 896/1680] output_blocks.0.1.transformer_blocks.3.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 897/1680] output_blocks.0.1.transformer_blocks.3.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 898/1680] output_blocks.0.1.transformer_blocks.3.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 899/1680] output_blocks.0.1.transformer_blocks.3.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 900/1680] output_blocks.0.1.transformer_blocks.3.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 901/1680] output_blocks.0.1.transformer_blocks.3.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 902/1680] output_blocks.0.1.transformer_blocks.3.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 903/1680] output_blocks.0.1.transformer_blocks.3.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 904/1680] output_blocks.0.1.transformer_blocks.3.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 905/1680] output_blocks.0.1.transformer_blocks.3.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 906/1680] output_blocks.0.1.transformer_blocks.3.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 907/1680] output_blocks.0.1.transformer_blocks.4.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 908/1680] output_blocks.0.1.transformer_blocks.4.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 909/1680] output_blocks.0.1.transformer_blocks.4.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 910/1680] output_blocks.0.1.transformer_blocks.4.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 911/1680] output_blocks.0.1.transformer_blocks.4.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 912/1680] output_blocks.0.1.transformer_blocks.4.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 913/1680] output_blocks.0.1.transformer_blocks.4.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 914/1680] output_blocks.0.1.transformer_blocks.4.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 915/1680] output_blocks.0.1.transformer_blocks.4.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 916/1680] output_blocks.0.1.transformer_blocks.4.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 917/1680] output_blocks.0.1.transformer_blocks.4.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 918/1680] output_blocks.0.1.transformer_blocks.4.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 919/1680] output_blocks.0.1.transformer_blocks.4.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 920/1680] output_blocks.0.1.transformer_blocks.4.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 921/1680] output_blocks.0.1.transformer_blocks.4.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 922/1680] output_blocks.0.1.transformer_blocks.4.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 923/1680] output_blocks.0.1.transformer_blocks.4.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 924/1680] output_blocks.0.1.transformer_blocks.4.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 925/1680] output_blocks.0.1.transformer_blocks.4.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 926/1680] output_blocks.0.1.transformer_blocks.4.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 927/1680] output_blocks.0.1.transformer_blocks.5.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 928/1680] output_blocks.0.1.transformer_blocks.5.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 929/1680] output_blocks.0.1.transformer_blocks.5.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 930/1680] output_blocks.0.1.transformer_blocks.5.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 931/1680] output_blocks.0.1.transformer_blocks.5.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 932/1680] output_blocks.0.1.transformer_blocks.5.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 933/1680] output_blocks.0.1.transformer_blocks.5.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 934/1680] output_blocks.0.1.transformer_blocks.5.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 935/1680] output_blocks.0.1.transformer_blocks.5.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 936/1680] output_blocks.0.1.transformer_blocks.5.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 937/1680] output_blocks.0.1.transformer_blocks.5.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 938/1680] output_blocks.0.1.transformer_blocks.5.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 939/1680] output_blocks.0.1.transformer_blocks.5.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 940/1680] output_blocks.0.1.transformer_blocks.5.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 941/1680] output_blocks.0.1.transformer_blocks.5.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 942/1680] output_blocks.0.1.transformer_blocks.5.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 943/1680] output_blocks.0.1.transformer_blocks.5.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 944/1680] output_blocks.0.1.transformer_blocks.5.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 945/1680] output_blocks.0.1.transformer_blocks.5.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 946/1680] output_blocks.0.1.transformer_blocks.5.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 947/1680] output_blocks.0.1.transformer_blocks.6.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 948/1680] output_blocks.0.1.transformer_blocks.6.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 949/1680] output_blocks.0.1.transformer_blocks.6.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 950/1680] output_blocks.0.1.transformer_blocks.6.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 951/1680] output_blocks.0.1.transformer_blocks.6.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 952/1680] output_blocks.0.1.transformer_blocks.6.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 953/1680] output_blocks.0.1.transformer_blocks.6.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 954/1680] output_blocks.0.1.transformer_blocks.6.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 955/1680] output_blocks.0.1.transformer_blocks.6.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 956/1680] output_blocks.0.1.transformer_blocks.6.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 957/1680] output_blocks.0.1.transformer_blocks.6.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 958/1680] output_blocks.0.1.transformer_blocks.6.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 959/1680] output_blocks.0.1.transformer_blocks.6.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 960/1680] output_blocks.0.1.transformer_blocks.6.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 961/1680] output_blocks.0.1.transformer_blocks.6.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 962/1680] output_blocks.0.1.transformer_blocks.6.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 963/1680] output_blocks.0.1.transformer_blocks.6.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 964/1680] output_blocks.0.1.transformer_blocks.6.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 965/1680] output_blocks.0.1.transformer_blocks.6.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 966/1680] output_blocks.0.1.transformer_blocks.6.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 967/1680] output_blocks.0.1.transformer_blocks.7.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 968/1680] output_blocks.0.1.transformer_blocks.7.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 969/1680] output_blocks.0.1.transformer_blocks.7.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 970/1680] output_blocks.0.1.transformer_blocks.7.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 971/1680] output_blocks.0.1.transformer_blocks.7.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 972/1680] output_blocks.0.1.transformer_blocks.7.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 973/1680] output_blocks.0.1.transformer_blocks.7.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 974/1680] output_blocks.0.1.transformer_blocks.7.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 975/1680] output_blocks.0.1.transformer_blocks.7.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 976/1680] output_blocks.0.1.transformer_blocks.7.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 977/1680] output_blocks.0.1.transformer_blocks.7.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 978/1680] output_blocks.0.1.transformer_blocks.7.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 979/1680] output_blocks.0.1.transformer_blocks.7.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 980/1680] output_blocks.0.1.transformer_blocks.7.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[ 981/1680] output_blocks.0.1.transformer_blocks.7.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 982/1680] output_blocks.0.1.transformer_blocks.7.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 983/1680] output_blocks.0.1.transformer_blocks.7.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 984/1680] output_blocks.0.1.transformer_blocks.7.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 985/1680] output_blocks.0.1.transformer_blocks.7.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 986/1680] output_blocks.0.1.transformer_blocks.7.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 987/1680] output_blocks.0.1.transformer_blocks.8.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 988/1680] output_blocks.0.1.transformer_blocks.8.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 989/1680] output_blocks.0.1.transformer_blocks.8.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 990/1680] output_blocks.0.1.transformer_blocks.8.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 991/1680] output_blocks.0.1.transformer_blocks.8.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[ 992/1680] output_blocks.0.1.transformer_blocks.8.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[ 993/1680] output_blocks.0.1.transformer_blocks.8.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[ 994/1680] output_blocks.0.1.transformer_blocks.8.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 995/1680] output_blocks.0.1.transformer_blocks.8.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[ 996/1680] output_blocks.0.1.transformer_blocks.8.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[ 997/1680] output_blocks.0.1.transformer_blocks.8.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[ 998/1680] output_blocks.0.1.transformer_blocks.8.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[ 999/1680] output_blocks.0.1.transformer_blocks.8.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1000/1680] output_blocks.0.1.transformer_blocks.8.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1001/1680] output_blocks.0.1.transformer_blocks.8.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1002/1680] output_blocks.0.1.transformer_blocks.8.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1003/1680] output_blocks.0.1.transformer_blocks.8.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1004/1680] output_blocks.0.1.transformer_blocks.8.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1005/1680] output_blocks.0.1.transformer_blocks.8.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1006/1680] output_blocks.0.1.transformer_blocks.8.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1007/1680] output_blocks.0.1.transformer_blocks.9.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1008/1680] output_blocks.0.1.transformer_blocks.9.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1009/1680] output_blocks.0.1.transformer_blocks.9.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1010/1680] output_blocks.0.1.transformer_blocks.9.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1011/1680] output_blocks.0.1.transformer_blocks.9.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1012/1680] output_blocks.0.1.transformer_blocks.9.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1013/1680] output_blocks.0.1.transformer_blocks.9.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1014/1680] output_blocks.0.1.transformer_blocks.9.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1015/1680] output_blocks.0.1.transformer_blocks.9.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1016/1680] output_blocks.0.1.transformer_blocks.9.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1017/1680] output_blocks.0.1.transformer_blocks.9.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1018/1680] output_blocks.0.1.transformer_blocks.9.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1019/1680] output_blocks.0.1.transformer_blocks.9.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1020/1680] output_blocks.0.1.transformer_blocks.9.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1021/1680] output_blocks.0.1.transformer_blocks.9.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1022/1680] output_blocks.0.1.transformer_blocks.9.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1023/1680] output_blocks.0.1.transformer_blocks.9.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1024/1680] output_blocks.0.1.transformer_blocks.9.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1025/1680] output_blocks.0.1.transformer_blocks.9.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1026/1680] output_blocks.0.1.transformer_blocks.9.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1027/1680]  output_blocks.1.0.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1028/1680] output_blocks.1.0.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1029/1680]   output_blocks.1.0.in_layers.0.bias - [ 2560,     1,     1,     1], type =    f16, size =    0.005 MB
[1030/1680] output_blocks.1.0.in_layers.0.weight - [ 2560,     1,     1,     1], type =    f16, size =    0.005 MB
[1031/1680]   output_blocks.1.0.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1032/1680] output_blocks.1.0.in_layers.2.weight - [    3,     3,  2560,  1280], type =    f16, size =   56.250 MB
[1033/1680]  output_blocks.1.0.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1034/1680] output_blocks.1.0.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1035/1680]  output_blocks.1.0.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1036/1680] output_blocks.1.0.out_layers.3.weight - [    3,     3,  1280,  1280], type =    f16, size =   28.125 MB
[1037/1680] output_blocks.1.0.skip_connection.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1038/1680] output_blocks.1.0.skip_connection.weight - [    1,     1,  2560,  1280], type =    f16, size =    6.250 MB
[1039/1680]          output_blocks.1.1.norm.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1040/1680]        output_blocks.1.1.norm.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1041/1680]       output_blocks.1.1.proj_in.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1042/1680]     output_blocks.1.1.proj_in.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1043/1680]      output_blocks.1.1.proj_out.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1044/1680]    output_blocks.1.1.proj_out.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1045/1680] output_blocks.1.1.transformer_blocks.0.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1046/1680] output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1047/1680] output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1048/1680] output_blocks.1.1.transformer_blocks.0.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1049/1680] output_blocks.1.1.transformer_blocks.0.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1050/1680] output_blocks.1.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1051/1680] output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1052/1680] output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1053/1680] output_blocks.1.1.transformer_blocks.0.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1054/1680] output_blocks.1.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1055/1680] output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1056/1680] output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1057/1680] output_blocks.1.1.transformer_blocks.0.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1058/1680] output_blocks.1.1.transformer_blocks.0.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1059/1680] output_blocks.1.1.transformer_blocks.0.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1060/1680] output_blocks.1.1.transformer_blocks.0.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1061/1680] output_blocks.1.1.transformer_blocks.0.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1062/1680] output_blocks.1.1.transformer_blocks.0.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1063/1680] output_blocks.1.1.transformer_blocks.0.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1064/1680] output_blocks.1.1.transformer_blocks.0.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1065/1680] output_blocks.1.1.transformer_blocks.1.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1066/1680] output_blocks.1.1.transformer_blocks.1.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1067/1680] output_blocks.1.1.transformer_blocks.1.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1068/1680] output_blocks.1.1.transformer_blocks.1.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1069/1680] output_blocks.1.1.transformer_blocks.1.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1070/1680] output_blocks.1.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1071/1680] output_blocks.1.1.transformer_blocks.1.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1072/1680] output_blocks.1.1.transformer_blocks.1.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1073/1680] output_blocks.1.1.transformer_blocks.1.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1074/1680] output_blocks.1.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1075/1680] output_blocks.1.1.transformer_blocks.1.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1076/1680] output_blocks.1.1.transformer_blocks.1.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1077/1680] output_blocks.1.1.transformer_blocks.1.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1078/1680] output_blocks.1.1.transformer_blocks.1.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1079/1680] output_blocks.1.1.transformer_blocks.1.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1080/1680] output_blocks.1.1.transformer_blocks.1.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1081/1680] output_blocks.1.1.transformer_blocks.1.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1082/1680] output_blocks.1.1.transformer_blocks.1.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1083/1680] output_blocks.1.1.transformer_blocks.1.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1084/1680] output_blocks.1.1.transformer_blocks.1.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1085/1680] output_blocks.1.1.transformer_blocks.2.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1086/1680] output_blocks.1.1.transformer_blocks.2.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1087/1680] output_blocks.1.1.transformer_blocks.2.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1088/1680] output_blocks.1.1.transformer_blocks.2.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1089/1680] output_blocks.1.1.transformer_blocks.2.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1090/1680] output_blocks.1.1.transformer_blocks.2.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1091/1680] output_blocks.1.1.transformer_blocks.2.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1092/1680] output_blocks.1.1.transformer_blocks.2.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1093/1680] output_blocks.1.1.transformer_blocks.2.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1094/1680] output_blocks.1.1.transformer_blocks.2.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1095/1680] output_blocks.1.1.transformer_blocks.2.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1096/1680] output_blocks.1.1.transformer_blocks.2.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1097/1680] output_blocks.1.1.transformer_blocks.2.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1098/1680] output_blocks.1.1.transformer_blocks.2.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1099/1680] output_blocks.1.1.transformer_blocks.2.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1100/1680] output_blocks.1.1.transformer_blocks.2.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1101/1680] output_blocks.1.1.transformer_blocks.2.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1102/1680] output_blocks.1.1.transformer_blocks.2.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1103/1680] output_blocks.1.1.transformer_blocks.2.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1104/1680] output_blocks.1.1.transformer_blocks.2.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1105/1680] output_blocks.1.1.transformer_blocks.3.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1106/1680] output_blocks.1.1.transformer_blocks.3.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1107/1680] output_blocks.1.1.transformer_blocks.3.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1108/1680] output_blocks.1.1.transformer_blocks.3.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1109/1680] output_blocks.1.1.transformer_blocks.3.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1110/1680] output_blocks.1.1.transformer_blocks.3.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1111/1680] output_blocks.1.1.transformer_blocks.3.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1112/1680] output_blocks.1.1.transformer_blocks.3.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1113/1680] output_blocks.1.1.transformer_blocks.3.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1114/1680] output_blocks.1.1.transformer_blocks.3.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1115/1680] output_blocks.1.1.transformer_blocks.3.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1116/1680] output_blocks.1.1.transformer_blocks.3.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1117/1680] output_blocks.1.1.transformer_blocks.3.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1118/1680] output_blocks.1.1.transformer_blocks.3.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1119/1680] output_blocks.1.1.transformer_blocks.3.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1120/1680] output_blocks.1.1.transformer_blocks.3.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1121/1680] output_blocks.1.1.transformer_blocks.3.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1122/1680] output_blocks.1.1.transformer_blocks.3.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1123/1680] output_blocks.1.1.transformer_blocks.3.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1124/1680] output_blocks.1.1.transformer_blocks.3.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1125/1680] output_blocks.1.1.transformer_blocks.4.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1126/1680] output_blocks.1.1.transformer_blocks.4.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1127/1680] output_blocks.1.1.transformer_blocks.4.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1128/1680] output_blocks.1.1.transformer_blocks.4.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1129/1680] output_blocks.1.1.transformer_blocks.4.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1130/1680] output_blocks.1.1.transformer_blocks.4.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1131/1680] output_blocks.1.1.transformer_blocks.4.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1132/1680] output_blocks.1.1.transformer_blocks.4.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1133/1680] output_blocks.1.1.transformer_blocks.4.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1134/1680] output_blocks.1.1.transformer_blocks.4.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1135/1680] output_blocks.1.1.transformer_blocks.4.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1136/1680] output_blocks.1.1.transformer_blocks.4.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1137/1680] output_blocks.1.1.transformer_blocks.4.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1138/1680] output_blocks.1.1.transformer_blocks.4.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1139/1680] output_blocks.1.1.transformer_blocks.4.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1140/1680] output_blocks.1.1.transformer_blocks.4.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1141/1680] output_blocks.1.1.transformer_blocks.4.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1142/1680] output_blocks.1.1.transformer_blocks.4.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1143/1680] output_blocks.1.1.transformer_blocks.4.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1144/1680] output_blocks.1.1.transformer_blocks.4.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1145/1680] output_blocks.1.1.transformer_blocks.5.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1146/1680] output_blocks.1.1.transformer_blocks.5.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1147/1680] output_blocks.1.1.transformer_blocks.5.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1148/1680] output_blocks.1.1.transformer_blocks.5.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1149/1680] output_blocks.1.1.transformer_blocks.5.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1150/1680] output_blocks.1.1.transformer_blocks.5.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1151/1680] output_blocks.1.1.transformer_blocks.5.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1152/1680] output_blocks.1.1.transformer_blocks.5.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1153/1680] output_blocks.1.1.transformer_blocks.5.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1154/1680] output_blocks.1.1.transformer_blocks.5.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1155/1680] output_blocks.1.1.transformer_blocks.5.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1156/1680] output_blocks.1.1.transformer_blocks.5.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1157/1680] output_blocks.1.1.transformer_blocks.5.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1158/1680] output_blocks.1.1.transformer_blocks.5.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1159/1680] output_blocks.1.1.transformer_blocks.5.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1160/1680] output_blocks.1.1.transformer_blocks.5.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1161/1680] output_blocks.1.1.transformer_blocks.5.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1162/1680] output_blocks.1.1.transformer_blocks.5.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1163/1680] output_blocks.1.1.transformer_blocks.5.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1164/1680] output_blocks.1.1.transformer_blocks.5.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1165/1680] output_blocks.1.1.transformer_blocks.6.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1166/1680] output_blocks.1.1.transformer_blocks.6.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1167/1680] output_blocks.1.1.transformer_blocks.6.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1168/1680] output_blocks.1.1.transformer_blocks.6.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1169/1680] output_blocks.1.1.transformer_blocks.6.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1170/1680] output_blocks.1.1.transformer_blocks.6.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1171/1680] output_blocks.1.1.transformer_blocks.6.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1172/1680] output_blocks.1.1.transformer_blocks.6.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1173/1680] output_blocks.1.1.transformer_blocks.6.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1174/1680] output_blocks.1.1.transformer_blocks.6.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1175/1680] output_blocks.1.1.transformer_blocks.6.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1176/1680] output_blocks.1.1.transformer_blocks.6.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1177/1680] output_blocks.1.1.transformer_blocks.6.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1178/1680] output_blocks.1.1.transformer_blocks.6.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1179/1680] output_blocks.1.1.transformer_blocks.6.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1180/1680] output_blocks.1.1.transformer_blocks.6.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1181/1680] output_blocks.1.1.transformer_blocks.6.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1182/1680] output_blocks.1.1.transformer_blocks.6.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1183/1680] output_blocks.1.1.transformer_blocks.6.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1184/1680] output_blocks.1.1.transformer_blocks.6.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1185/1680] output_blocks.1.1.transformer_blocks.7.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1186/1680] output_blocks.1.1.transformer_blocks.7.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1187/1680] output_blocks.1.1.transformer_blocks.7.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1188/1680] output_blocks.1.1.transformer_blocks.7.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1189/1680] output_blocks.1.1.transformer_blocks.7.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1190/1680] output_blocks.1.1.transformer_blocks.7.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1191/1680] output_blocks.1.1.transformer_blocks.7.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1192/1680] output_blocks.1.1.transformer_blocks.7.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1193/1680] output_blocks.1.1.transformer_blocks.7.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1194/1680] output_blocks.1.1.transformer_blocks.7.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1195/1680] output_blocks.1.1.transformer_blocks.7.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1196/1680] output_blocks.1.1.transformer_blocks.7.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1197/1680] output_blocks.1.1.transformer_blocks.7.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1198/1680] output_blocks.1.1.transformer_blocks.7.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1199/1680] output_blocks.1.1.transformer_blocks.7.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1200/1680] output_blocks.1.1.transformer_blocks.7.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1201/1680] output_blocks.1.1.transformer_blocks.7.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1202/1680] output_blocks.1.1.transformer_blocks.7.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1203/1680] output_blocks.1.1.transformer_blocks.7.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1204/1680] output_blocks.1.1.transformer_blocks.7.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1205/1680] output_blocks.1.1.transformer_blocks.8.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1206/1680] output_blocks.1.1.transformer_blocks.8.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1207/1680] output_blocks.1.1.transformer_blocks.8.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1208/1680] output_blocks.1.1.transformer_blocks.8.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1209/1680] output_blocks.1.1.transformer_blocks.8.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1210/1680] output_blocks.1.1.transformer_blocks.8.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1211/1680] output_blocks.1.1.transformer_blocks.8.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1212/1680] output_blocks.1.1.transformer_blocks.8.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1213/1680] output_blocks.1.1.transformer_blocks.8.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1214/1680] output_blocks.1.1.transformer_blocks.8.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1215/1680] output_blocks.1.1.transformer_blocks.8.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1216/1680] output_blocks.1.1.transformer_blocks.8.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1217/1680] output_blocks.1.1.transformer_blocks.8.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1218/1680] output_blocks.1.1.transformer_blocks.8.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1219/1680] output_blocks.1.1.transformer_blocks.8.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1220/1680] output_blocks.1.1.transformer_blocks.8.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1221/1680] output_blocks.1.1.transformer_blocks.8.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1222/1680] output_blocks.1.1.transformer_blocks.8.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1223/1680] output_blocks.1.1.transformer_blocks.8.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1224/1680] output_blocks.1.1.transformer_blocks.8.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1225/1680] output_blocks.1.1.transformer_blocks.9.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1226/1680] output_blocks.1.1.transformer_blocks.9.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1227/1680] output_blocks.1.1.transformer_blocks.9.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1228/1680] output_blocks.1.1.transformer_blocks.9.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1229/1680] output_blocks.1.1.transformer_blocks.9.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1230/1680] output_blocks.1.1.transformer_blocks.9.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1231/1680] output_blocks.1.1.transformer_blocks.9.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1232/1680] output_blocks.1.1.transformer_blocks.9.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1233/1680] output_blocks.1.1.transformer_blocks.9.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1234/1680] output_blocks.1.1.transformer_blocks.9.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1235/1680] output_blocks.1.1.transformer_blocks.9.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1236/1680] output_blocks.1.1.transformer_blocks.9.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1237/1680] output_blocks.1.1.transformer_blocks.9.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1238/1680] output_blocks.1.1.transformer_blocks.9.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1239/1680] output_blocks.1.1.transformer_blocks.9.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1240/1680] output_blocks.1.1.transformer_blocks.9.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1241/1680] output_blocks.1.1.transformer_blocks.9.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1242/1680] output_blocks.1.1.transformer_blocks.9.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1243/1680] output_blocks.1.1.transformer_blocks.9.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1244/1680] output_blocks.1.1.transformer_blocks.9.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1245/1680]  output_blocks.2.0.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1246/1680] output_blocks.2.0.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1247/1680]   output_blocks.2.0.in_layers.0.bias - [ 1920,     1,     1,     1], type =    f16, size =    0.004 MB
[1248/1680] output_blocks.2.0.in_layers.0.weight - [ 1920,     1,     1,     1], type =    f16, size =    0.004 MB
[1249/1680]   output_blocks.2.0.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1250/1680] output_blocks.2.0.in_layers.2.weight - [    3,     3,  1920,  1280], type =    f16, size =   42.188 MB
[1251/1680]  output_blocks.2.0.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1252/1680] output_blocks.2.0.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1253/1680]  output_blocks.2.0.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1254/1680] output_blocks.2.0.out_layers.3.weight - [    3,     3,  1280,  1280], type =    f16, size =   28.125 MB
[1255/1680] output_blocks.2.0.skip_connection.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1256/1680] output_blocks.2.0.skip_connection.weight - [    1,     1,  1920,  1280], type =    f16, size =    4.688 MB
[1257/1680]          output_blocks.2.1.norm.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1258/1680]        output_blocks.2.1.norm.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1259/1680]       output_blocks.2.1.proj_in.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1260/1680]     output_blocks.2.1.proj_in.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1261/1680]      output_blocks.2.1.proj_out.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1262/1680]    output_blocks.2.1.proj_out.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1263/1680] output_blocks.2.1.transformer_blocks.0.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1264/1680] output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1265/1680] output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1266/1680] output_blocks.2.1.transformer_blocks.0.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1267/1680] output_blocks.2.1.transformer_blocks.0.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1268/1680] output_blocks.2.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1269/1680] output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1270/1680] output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1271/1680] output_blocks.2.1.transformer_blocks.0.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1272/1680] output_blocks.2.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1273/1680] output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1274/1680] output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1275/1680] output_blocks.2.1.transformer_blocks.0.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1276/1680] output_blocks.2.1.transformer_blocks.0.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1277/1680] output_blocks.2.1.transformer_blocks.0.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1278/1680] output_blocks.2.1.transformer_blocks.0.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1279/1680] output_blocks.2.1.transformer_blocks.0.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1280/1680] output_blocks.2.1.transformer_blocks.0.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1281/1680] output_blocks.2.1.transformer_blocks.0.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1282/1680] output_blocks.2.1.transformer_blocks.0.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1283/1680] output_blocks.2.1.transformer_blocks.1.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1284/1680] output_blocks.2.1.transformer_blocks.1.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1285/1680] output_blocks.2.1.transformer_blocks.1.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1286/1680] output_blocks.2.1.transformer_blocks.1.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1287/1680] output_blocks.2.1.transformer_blocks.1.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1288/1680] output_blocks.2.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1289/1680] output_blocks.2.1.transformer_blocks.1.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1290/1680] output_blocks.2.1.transformer_blocks.1.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1291/1680] output_blocks.2.1.transformer_blocks.1.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1292/1680] output_blocks.2.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1293/1680] output_blocks.2.1.transformer_blocks.1.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1294/1680] output_blocks.2.1.transformer_blocks.1.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1295/1680] output_blocks.2.1.transformer_blocks.1.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1296/1680] output_blocks.2.1.transformer_blocks.1.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1297/1680] output_blocks.2.1.transformer_blocks.1.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1298/1680] output_blocks.2.1.transformer_blocks.1.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1299/1680] output_blocks.2.1.transformer_blocks.1.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1300/1680] output_blocks.2.1.transformer_blocks.1.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1301/1680] output_blocks.2.1.transformer_blocks.1.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1302/1680] output_blocks.2.1.transformer_blocks.1.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1303/1680] output_blocks.2.1.transformer_blocks.2.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1304/1680] output_blocks.2.1.transformer_blocks.2.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1305/1680] output_blocks.2.1.transformer_blocks.2.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1306/1680] output_blocks.2.1.transformer_blocks.2.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1307/1680] output_blocks.2.1.transformer_blocks.2.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1308/1680] output_blocks.2.1.transformer_blocks.2.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1309/1680] output_blocks.2.1.transformer_blocks.2.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1310/1680] output_blocks.2.1.transformer_blocks.2.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1311/1680] output_blocks.2.1.transformer_blocks.2.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1312/1680] output_blocks.2.1.transformer_blocks.2.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1313/1680] output_blocks.2.1.transformer_blocks.2.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1314/1680] output_blocks.2.1.transformer_blocks.2.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1315/1680] output_blocks.2.1.transformer_blocks.2.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1316/1680] output_blocks.2.1.transformer_blocks.2.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1317/1680] output_blocks.2.1.transformer_blocks.2.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1318/1680] output_blocks.2.1.transformer_blocks.2.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1319/1680] output_blocks.2.1.transformer_blocks.2.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1320/1680] output_blocks.2.1.transformer_blocks.2.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1321/1680] output_blocks.2.1.transformer_blocks.2.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1322/1680] output_blocks.2.1.transformer_blocks.2.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1323/1680] output_blocks.2.1.transformer_blocks.3.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1324/1680] output_blocks.2.1.transformer_blocks.3.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1325/1680] output_blocks.2.1.transformer_blocks.3.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1326/1680] output_blocks.2.1.transformer_blocks.3.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1327/1680] output_blocks.2.1.transformer_blocks.3.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1328/1680] output_blocks.2.1.transformer_blocks.3.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1329/1680] output_blocks.2.1.transformer_blocks.3.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1330/1680] output_blocks.2.1.transformer_blocks.3.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1331/1680] output_blocks.2.1.transformer_blocks.3.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1332/1680] output_blocks.2.1.transformer_blocks.3.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1333/1680] output_blocks.2.1.transformer_blocks.3.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1334/1680] output_blocks.2.1.transformer_blocks.3.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1335/1680] output_blocks.2.1.transformer_blocks.3.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1336/1680] output_blocks.2.1.transformer_blocks.3.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1337/1680] output_blocks.2.1.transformer_blocks.3.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1338/1680] output_blocks.2.1.transformer_blocks.3.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1339/1680] output_blocks.2.1.transformer_blocks.3.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1340/1680] output_blocks.2.1.transformer_blocks.3.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1341/1680] output_blocks.2.1.transformer_blocks.3.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1342/1680] output_blocks.2.1.transformer_blocks.3.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1343/1680] output_blocks.2.1.transformer_blocks.4.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1344/1680] output_blocks.2.1.transformer_blocks.4.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1345/1680] output_blocks.2.1.transformer_blocks.4.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1346/1680] output_blocks.2.1.transformer_blocks.4.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1347/1680] output_blocks.2.1.transformer_blocks.4.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1348/1680] output_blocks.2.1.transformer_blocks.4.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1349/1680] output_blocks.2.1.transformer_blocks.4.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1350/1680] output_blocks.2.1.transformer_blocks.4.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1351/1680] output_blocks.2.1.transformer_blocks.4.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1352/1680] output_blocks.2.1.transformer_blocks.4.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1353/1680] output_blocks.2.1.transformer_blocks.4.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1354/1680] output_blocks.2.1.transformer_blocks.4.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1355/1680] output_blocks.2.1.transformer_blocks.4.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1356/1680] output_blocks.2.1.transformer_blocks.4.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1357/1680] output_blocks.2.1.transformer_blocks.4.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1358/1680] output_blocks.2.1.transformer_blocks.4.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1359/1680] output_blocks.2.1.transformer_blocks.4.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1360/1680] output_blocks.2.1.transformer_blocks.4.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1361/1680] output_blocks.2.1.transformer_blocks.4.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1362/1680] output_blocks.2.1.transformer_blocks.4.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1363/1680] output_blocks.2.1.transformer_blocks.5.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1364/1680] output_blocks.2.1.transformer_blocks.5.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1365/1680] output_blocks.2.1.transformer_blocks.5.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1366/1680] output_blocks.2.1.transformer_blocks.5.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1367/1680] output_blocks.2.1.transformer_blocks.5.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1368/1680] output_blocks.2.1.transformer_blocks.5.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1369/1680] output_blocks.2.1.transformer_blocks.5.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1370/1680] output_blocks.2.1.transformer_blocks.5.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1371/1680] output_blocks.2.1.transformer_blocks.5.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1372/1680] output_blocks.2.1.transformer_blocks.5.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1373/1680] output_blocks.2.1.transformer_blocks.5.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1374/1680] output_blocks.2.1.transformer_blocks.5.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1375/1680] output_blocks.2.1.transformer_blocks.5.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1376/1680] output_blocks.2.1.transformer_blocks.5.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1377/1680] output_blocks.2.1.transformer_blocks.5.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1378/1680] output_blocks.2.1.transformer_blocks.5.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1379/1680] output_blocks.2.1.transformer_blocks.5.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1380/1680] output_blocks.2.1.transformer_blocks.5.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1381/1680] output_blocks.2.1.transformer_blocks.5.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1382/1680] output_blocks.2.1.transformer_blocks.5.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1383/1680] output_blocks.2.1.transformer_blocks.6.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1384/1680] output_blocks.2.1.transformer_blocks.6.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1385/1680] output_blocks.2.1.transformer_blocks.6.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1386/1680] output_blocks.2.1.transformer_blocks.6.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1387/1680] output_blocks.2.1.transformer_blocks.6.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1388/1680] output_blocks.2.1.transformer_blocks.6.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1389/1680] output_blocks.2.1.transformer_blocks.6.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1390/1680] output_blocks.2.1.transformer_blocks.6.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1391/1680] output_blocks.2.1.transformer_blocks.6.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1392/1680] output_blocks.2.1.transformer_blocks.6.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1393/1680] output_blocks.2.1.transformer_blocks.6.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1394/1680] output_blocks.2.1.transformer_blocks.6.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1395/1680] output_blocks.2.1.transformer_blocks.6.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1396/1680] output_blocks.2.1.transformer_blocks.6.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1397/1680] output_blocks.2.1.transformer_blocks.6.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1398/1680] output_blocks.2.1.transformer_blocks.6.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1399/1680] output_blocks.2.1.transformer_blocks.6.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1400/1680] output_blocks.2.1.transformer_blocks.6.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1401/1680] output_blocks.2.1.transformer_blocks.6.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1402/1680] output_blocks.2.1.transformer_blocks.6.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1403/1680] output_blocks.2.1.transformer_blocks.7.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1404/1680] output_blocks.2.1.transformer_blocks.7.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1405/1680] output_blocks.2.1.transformer_blocks.7.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1406/1680] output_blocks.2.1.transformer_blocks.7.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1407/1680] output_blocks.2.1.transformer_blocks.7.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1408/1680] output_blocks.2.1.transformer_blocks.7.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1409/1680] output_blocks.2.1.transformer_blocks.7.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1410/1680] output_blocks.2.1.transformer_blocks.7.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1411/1680] output_blocks.2.1.transformer_blocks.7.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1412/1680] output_blocks.2.1.transformer_blocks.7.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1413/1680] output_blocks.2.1.transformer_blocks.7.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1414/1680] output_blocks.2.1.transformer_blocks.7.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1415/1680] output_blocks.2.1.transformer_blocks.7.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1416/1680] output_blocks.2.1.transformer_blocks.7.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1417/1680] output_blocks.2.1.transformer_blocks.7.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1418/1680] output_blocks.2.1.transformer_blocks.7.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1419/1680] output_blocks.2.1.transformer_blocks.7.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1420/1680] output_blocks.2.1.transformer_blocks.7.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1421/1680] output_blocks.2.1.transformer_blocks.7.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1422/1680] output_blocks.2.1.transformer_blocks.7.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1423/1680] output_blocks.2.1.transformer_blocks.8.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1424/1680] output_blocks.2.1.transformer_blocks.8.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1425/1680] output_blocks.2.1.transformer_blocks.8.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1426/1680] output_blocks.2.1.transformer_blocks.8.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1427/1680] output_blocks.2.1.transformer_blocks.8.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1428/1680] output_blocks.2.1.transformer_blocks.8.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1429/1680] output_blocks.2.1.transformer_blocks.8.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1430/1680] output_blocks.2.1.transformer_blocks.8.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1431/1680] output_blocks.2.1.transformer_blocks.8.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1432/1680] output_blocks.2.1.transformer_blocks.8.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1433/1680] output_blocks.2.1.transformer_blocks.8.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1434/1680] output_blocks.2.1.transformer_blocks.8.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1435/1680] output_blocks.2.1.transformer_blocks.8.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1436/1680] output_blocks.2.1.transformer_blocks.8.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1437/1680] output_blocks.2.1.transformer_blocks.8.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1438/1680] output_blocks.2.1.transformer_blocks.8.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1439/1680] output_blocks.2.1.transformer_blocks.8.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1440/1680] output_blocks.2.1.transformer_blocks.8.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1441/1680] output_blocks.2.1.transformer_blocks.8.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1442/1680] output_blocks.2.1.transformer_blocks.8.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1443/1680] output_blocks.2.1.transformer_blocks.9.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1444/1680] output_blocks.2.1.transformer_blocks.9.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1445/1680] output_blocks.2.1.transformer_blocks.9.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1446/1680] output_blocks.2.1.transformer_blocks.9.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1447/1680] output_blocks.2.1.transformer_blocks.9.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1448/1680] output_blocks.2.1.transformer_blocks.9.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB
[1449/1680] output_blocks.2.1.transformer_blocks.9.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1450/1680] output_blocks.2.1.transformer_blocks.9.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1451/1680] output_blocks.2.1.transformer_blocks.9.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB
[1452/1680] output_blocks.2.1.transformer_blocks.9.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB
[1453/1680] output_blocks.2.1.transformer_blocks.9.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB
[1454/1680] output_blocks.2.1.transformer_blocks.9.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB
[1455/1680] output_blocks.2.1.transformer_blocks.9.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1456/1680] output_blocks.2.1.transformer_blocks.9.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB
[1457/1680] output_blocks.2.1.transformer_blocks.9.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1458/1680] output_blocks.2.1.transformer_blocks.9.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1459/1680] output_blocks.2.1.transformer_blocks.9.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1460/1680] output_blocks.2.1.transformer_blocks.9.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1461/1680] output_blocks.2.1.transformer_blocks.9.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1462/1680] output_blocks.2.1.transformer_blocks.9.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1463/1680]          output_blocks.2.2.conv.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1464/1680]        output_blocks.2.2.conv.weight - [    3,     3,  1280,  1280], type =    f16, size =   28.125 MB
[1465/1680]  output_blocks.3.0.emb_layers.1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1466/1680] output_blocks.3.0.emb_layers.1.weight - [ 1280,   640,     1,     1], type =    f16, converting to q5_K .. size =     1.56 MiB ->     0.54 MiB
[1467/1680]   output_blocks.3.0.in_layers.0.bias - [ 1920,     1,     1,     1], type =    f16, size =    0.004 MB
[1468/1680] output_blocks.3.0.in_layers.0.weight - [ 1920,     1,     1,     1], type =    f16, size =    0.004 MB
[1469/1680]   output_blocks.3.0.in_layers.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1470/1680] output_blocks.3.0.in_layers.2.weight - [    3,     3,  1920,   640], type =    f16, size =   21.094 MB
[1471/1680]  output_blocks.3.0.out_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1472/1680] output_blocks.3.0.out_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1473/1680]  output_blocks.3.0.out_layers.3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1474/1680] output_blocks.3.0.out_layers.3.weight - [    3,     3,   640,   640], type =    f16, size =    7.031 MB
[1475/1680] output_blocks.3.0.skip_connection.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1476/1680] output_blocks.3.0.skip_connection.weight - [    1,     1,  1920,   640], type =    f16, size =    2.344 MB
[1477/1680]          output_blocks.3.1.norm.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1478/1680]        output_blocks.3.1.norm.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1479/1680]       output_blocks.3.1.proj_in.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1480/1680]     output_blocks.3.1.proj_in.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1481/1680]      output_blocks.3.1.proj_out.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1482/1680]    output_blocks.3.1.proj_out.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1483/1680] output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1484/1680] output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1485/1680] output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1486/1680] output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1487/1680] output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.78 MiB ->     0.42 MiB
[1488/1680] output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB
[1489/1680] output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1490/1680] output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1491/1680] output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1492/1680] output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q6_K .. size =     2.50 MiB ->     1.03 MiB
[1493/1680] output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB
[1494/1680] output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight - [  640,  5120,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 5120 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     6.25 MiB ->     2.34 MiB
[1495/1680] output_blocks.3.1.transformer_blocks.0.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1496/1680] output_blocks.3.1.transformer_blocks.0.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1497/1680] output_blocks.3.1.transformer_blocks.0.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1498/1680] output_blocks.3.1.transformer_blocks.0.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1499/1680] output_blocks.3.1.transformer_blocks.0.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1500/1680] output_blocks.3.1.transformer_blocks.0.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1501/1680] output_blocks.3.1.transformer_blocks.0.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1502/1680] output_blocks.3.1.transformer_blocks.0.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1503/1680] output_blocks.3.1.transformer_blocks.1.attn1.to_k.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1504/1680] output_blocks.3.1.transformer_blocks.1.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1505/1680] output_blocks.3.1.transformer_blocks.1.attn1.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1506/1680] output_blocks.3.1.transformer_blocks.1.attn1.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1507/1680] output_blocks.3.1.transformer_blocks.1.attn1.to_v.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.78 MiB ->     0.42 MiB
[1508/1680] output_blocks.3.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB
[1509/1680] output_blocks.3.1.transformer_blocks.1.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1510/1680] output_blocks.3.1.transformer_blocks.1.attn2.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1511/1680] output_blocks.3.1.transformer_blocks.1.attn2.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1512/1680] output_blocks.3.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q6_K .. size =     2.50 MiB ->     1.03 MiB
[1513/1680] output_blocks.3.1.transformer_blocks.1.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB
[1514/1680] output_blocks.3.1.transformer_blocks.1.ff.net.0.proj.weight - [  640,  5120,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 5120 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     6.25 MiB ->     2.34 MiB
[1515/1680] output_blocks.3.1.transformer_blocks.1.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1516/1680] output_blocks.3.1.transformer_blocks.1.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1517/1680] output_blocks.3.1.transformer_blocks.1.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1518/1680] output_blocks.3.1.transformer_blocks.1.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1519/1680] output_blocks.3.1.transformer_blocks.1.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1520/1680] output_blocks.3.1.transformer_blocks.1.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1521/1680] output_blocks.3.1.transformer_blocks.1.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1522/1680] output_blocks.3.1.transformer_blocks.1.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1523/1680]  output_blocks.4.0.emb_layers.1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1524/1680] output_blocks.4.0.emb_layers.1.weight - [ 1280,   640,     1,     1], type =    f16, converting to q5_K .. size =     1.56 MiB ->     0.54 MiB
[1525/1680]   output_blocks.4.0.in_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1526/1680] output_blocks.4.0.in_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1527/1680]   output_blocks.4.0.in_layers.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1528/1680] output_blocks.4.0.in_layers.2.weight - [    3,     3,  1280,   640], type =    f16, size =   14.062 MB
[1529/1680]  output_blocks.4.0.out_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1530/1680] output_blocks.4.0.out_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1531/1680]  output_blocks.4.0.out_layers.3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1532/1680] output_blocks.4.0.out_layers.3.weight - [    3,     3,   640,   640], type =    f16, size =    7.031 MB
[1533/1680] output_blocks.4.0.skip_connection.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1534/1680] output_blocks.4.0.skip_connection.weight - [    1,     1,  1280,   640], type =    f16, size =    1.562 MB
[1535/1680]          output_blocks.4.1.norm.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1536/1680]        output_blocks.4.1.norm.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1537/1680]       output_blocks.4.1.proj_in.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1538/1680]     output_blocks.4.1.proj_in.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1539/1680]      output_blocks.4.1.proj_out.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1540/1680]    output_blocks.4.1.proj_out.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1541/1680] output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1542/1680] output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1543/1680] output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1544/1680] output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1545/1680] output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.78 MiB ->     0.42 MiB
[1546/1680] output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB
[1547/1680] output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1548/1680] output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1549/1680] output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1550/1680] output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q6_K .. size =     2.50 MiB ->     1.03 MiB
[1551/1680] output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB
[1552/1680] output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight - [  640,  5120,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 5120 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     6.25 MiB ->     2.34 MiB
[1553/1680] output_blocks.4.1.transformer_blocks.0.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1554/1680] output_blocks.4.1.transformer_blocks.0.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1555/1680] output_blocks.4.1.transformer_blocks.0.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1556/1680] output_blocks.4.1.transformer_blocks.0.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1557/1680] output_blocks.4.1.transformer_blocks.0.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1558/1680] output_blocks.4.1.transformer_blocks.0.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1559/1680] output_blocks.4.1.transformer_blocks.0.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1560/1680] output_blocks.4.1.transformer_blocks.0.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1561/1680] output_blocks.4.1.transformer_blocks.1.attn1.to_k.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1562/1680] output_blocks.4.1.transformer_blocks.1.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1563/1680] output_blocks.4.1.transformer_blocks.1.attn1.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1564/1680] output_blocks.4.1.transformer_blocks.1.attn1.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1565/1680] output_blocks.4.1.transformer_blocks.1.attn1.to_v.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.78 MiB ->     0.42 MiB
[1566/1680] output_blocks.4.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB
[1567/1680] output_blocks.4.1.transformer_blocks.1.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1568/1680] output_blocks.4.1.transformer_blocks.1.attn2.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1569/1680] output_blocks.4.1.transformer_blocks.1.attn2.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1570/1680] output_blocks.4.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q6_K .. size =     2.50 MiB ->     1.03 MiB
[1571/1680] output_blocks.4.1.transformer_blocks.1.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB
[1572/1680] output_blocks.4.1.transformer_blocks.1.ff.net.0.proj.weight - [  640,  5120,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 5120 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     6.25 MiB ->     2.34 MiB
[1573/1680] output_blocks.4.1.transformer_blocks.1.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1574/1680] output_blocks.4.1.transformer_blocks.1.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1575/1680] output_blocks.4.1.transformer_blocks.1.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1576/1680] output_blocks.4.1.transformer_blocks.1.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1577/1680] output_blocks.4.1.transformer_blocks.1.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1578/1680] output_blocks.4.1.transformer_blocks.1.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1579/1680] output_blocks.4.1.transformer_blocks.1.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1580/1680] output_blocks.4.1.transformer_blocks.1.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1581/1680]  output_blocks.5.0.emb_layers.1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1582/1680] output_blocks.5.0.emb_layers.1.weight - [ 1280,   640,     1,     1], type =    f16, converting to q5_K .. size =     1.56 MiB ->     0.54 MiB
[1583/1680]   output_blocks.5.0.in_layers.0.bias - [  960,     1,     1,     1], type =    f16, size =    0.002 MB
[1584/1680] output_blocks.5.0.in_layers.0.weight - [  960,     1,     1,     1], type =    f16, size =    0.002 MB
[1585/1680]   output_blocks.5.0.in_layers.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1586/1680] output_blocks.5.0.in_layers.2.weight - [    3,     3,   960,   640], type =    f16, size =   10.547 MB
[1587/1680]  output_blocks.5.0.out_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1588/1680] output_blocks.5.0.out_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1589/1680]  output_blocks.5.0.out_layers.3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1590/1680] output_blocks.5.0.out_layers.3.weight - [    3,     3,   640,   640], type =    f16, size =    7.031 MB
[1591/1680] output_blocks.5.0.skip_connection.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1592/1680] output_blocks.5.0.skip_connection.weight - [    1,     1,   960,   640], type =    f16, size =    1.172 MB
[1593/1680]          output_blocks.5.1.norm.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1594/1680]        output_blocks.5.1.norm.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1595/1680]       output_blocks.5.1.proj_in.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1596/1680]     output_blocks.5.1.proj_in.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1597/1680]      output_blocks.5.1.proj_out.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1598/1680]    output_blocks.5.1.proj_out.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1599/1680] output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1600/1680] output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1601/1680] output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1602/1680] output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1603/1680] output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.78 MiB ->     0.42 MiB
[1604/1680] output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB
[1605/1680] output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1606/1680] output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1607/1680] output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1608/1680] output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q6_K .. size =     2.50 MiB ->     1.03 MiB
[1609/1680] output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB
[1610/1680] output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight - [  640,  5120,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 5120 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     6.25 MiB ->     2.34 MiB
[1611/1680] output_blocks.5.1.transformer_blocks.0.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1612/1680] output_blocks.5.1.transformer_blocks.0.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1613/1680] output_blocks.5.1.transformer_blocks.0.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1614/1680] output_blocks.5.1.transformer_blocks.0.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1615/1680] output_blocks.5.1.transformer_blocks.0.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1616/1680] output_blocks.5.1.transformer_blocks.0.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1617/1680] output_blocks.5.1.transformer_blocks.0.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1618/1680] output_blocks.5.1.transformer_blocks.0.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1619/1680] output_blocks.5.1.transformer_blocks.1.attn1.to_k.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1620/1680] output_blocks.5.1.transformer_blocks.1.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1621/1680] output_blocks.5.1.transformer_blocks.1.attn1.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1622/1680] output_blocks.5.1.transformer_blocks.1.attn1.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1623/1680] output_blocks.5.1.transformer_blocks.1.attn1.to_v.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.78 MiB ->     0.42 MiB
[1624/1680] output_blocks.5.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB
[1625/1680] output_blocks.5.1.transformer_blocks.1.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1626/1680] output_blocks.5.1.transformer_blocks.1.attn2.to_out.0.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1627/1680] output_blocks.5.1.transformer_blocks.1.attn2.to_q.weight - [  640,   640,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 640 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     0.78 MiB ->     0.29 MiB
[1628/1680] output_blocks.5.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q6_K .. size =     2.50 MiB ->     1.03 MiB
[1629/1680] output_blocks.5.1.transformer_blocks.1.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB
[1630/1680] output_blocks.5.1.transformer_blocks.1.ff.net.0.proj.weight - [  640,  5120,     1,     1], type =    f16, 

img_tensor_get_type : tensor cols 640 x 5120 are not divisible by 256, required for q5_K - using fallback quantization q5_1
converting to q5_1 .. size =     6.25 MiB ->     2.34 MiB
[1631/1680] output_blocks.5.1.transformer_blocks.1.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1632/1680] output_blocks.5.1.transformer_blocks.1.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB
[1633/1680] output_blocks.5.1.transformer_blocks.1.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1634/1680] output_blocks.5.1.transformer_blocks.1.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1635/1680] output_blocks.5.1.transformer_blocks.1.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1636/1680] output_blocks.5.1.transformer_blocks.1.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1637/1680] output_blocks.5.1.transformer_blocks.1.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1638/1680] output_blocks.5.1.transformer_blocks.1.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1639/1680]          output_blocks.5.2.conv.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1640/1680]        output_blocks.5.2.conv.weight - [    3,     3,   640,   640], type =    f16, size =    7.031 MB
[1641/1680]  output_blocks.6.0.emb_layers.1.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1642/1680] output_blocks.6.0.emb_layers.1.weight - [ 1280,   320,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB
[1643/1680]   output_blocks.6.0.in_layers.0.bias - [  960,     1,     1,     1], type =    f16, size =    0.002 MB
[1644/1680] output_blocks.6.0.in_layers.0.weight - [  960,     1,     1,     1], type =    f16, size =    0.002 MB
[1645/1680]   output_blocks.6.0.in_layers.2.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1646/1680] output_blocks.6.0.in_layers.2.weight - [    3,     3,   960,   320], type =    f16, size =    5.273 MB
[1647/1680]  output_blocks.6.0.out_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1648/1680] output_blocks.6.0.out_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1649/1680]  output_blocks.6.0.out_layers.3.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1650/1680] output_blocks.6.0.out_layers.3.weight - [    3,     3,   320,   320], type =    f16, size =    1.758 MB
[1651/1680] output_blocks.6.0.skip_connection.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1652/1680] output_blocks.6.0.skip_connection.weight - [    1,     1,   960,   320], type =    f16, size =    0.586 MB
[1653/1680]  output_blocks.7.0.emb_layers.1.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1654/1680] output_blocks.7.0.emb_layers.1.weight - [ 1280,   320,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB
[1655/1680]   output_blocks.7.0.in_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1656/1680] output_blocks.7.0.in_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1657/1680]   output_blocks.7.0.in_layers.2.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1658/1680] output_blocks.7.0.in_layers.2.weight - [    3,     3,   640,   320], type =    f16, size =    3.516 MB
[1659/1680]  output_blocks.7.0.out_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1660/1680] output_blocks.7.0.out_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1661/1680]  output_blocks.7.0.out_layers.3.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1662/1680] output_blocks.7.0.out_layers.3.weight - [    3,     3,   320,   320], type =    f16, size =    1.758 MB
[1663/1680] output_blocks.7.0.skip_connection.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1664/1680] output_blocks.7.0.skip_connection.weight - [    1,     1,   640,   320], type =    f16, size =    0.391 MB
[1665/1680]  output_blocks.8.0.emb_layers.1.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1666/1680] output_blocks.8.0.emb_layers.1.weight - [ 1280,   320,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB
[1667/1680]   output_blocks.8.0.in_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1668/1680] output_blocks.8.0.in_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB
[1669/1680]   output_blocks.8.0.in_layers.2.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1670/1680] output_blocks.8.0.in_layers.2.weight - [    3,     3,   640,   320], type =    f16, size =    3.516 MB
[1671/1680]  output_blocks.8.0.out_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1672/1680] output_blocks.8.0.out_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1673/1680]  output_blocks.8.0.out_layers.3.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1674/1680] output_blocks.8.0.out_layers.3.weight - [    3,     3,   320,   320], type =    f16, size =    1.758 MB
[1675/1680] output_blocks.8.0.skip_connection.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB
[1676/1680] output_blocks.8.0.skip_connection.weight - [    1,     1,   640,   320], type =    f16, size =    0.391 MB
[1677/1680]                    time_embed.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1678/1680]                  time_embed.0.weight - [  320,  1280,     1,     1], type =    f16, size =    0.781 MB
[1679/1680]                    time_embed.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB
[1680/1680]                  time_embed.2.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB
llama_model_quantize_internal: model size  =  4897.05 MB
llama_model_quantize_internal: quant size  =  2202.37 MB
llama_model_quantize_internal: WARNING: 80 of 739 tensor(s) required fallback quantization

main: quantize time = 126035.26 ms
main:    total time = 126035.27 ms
